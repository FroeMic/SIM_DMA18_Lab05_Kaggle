{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Classification #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel, chi2, f_classif\n",
    "\n",
    "# models\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "# import xgboost\n",
    "\n",
    "# gridsearch and pipelining\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid, KFold\n",
    "\n",
    "# sklearn utils\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Name constants for file locations. Assuming files in same folder as notebook\n",
    "TRAIN_FILE = 'Lab5_train.csv'\n",
    "TEST_FILE = 'Lab5_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_df(PATH):\n",
    "    '''Function to read a file into a CSV and return it'''\n",
    "    df_original = pd.read_csv(PATH)\n",
    "    return df_original\n",
    "\n",
    "get_original_df(TRAIN_FILE).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(name):\n",
    "    '''Adding granular mapping for names'''\n",
    "    title_lookup = {\n",
    "                    'Mr':'Mr', 'Mrs':'Mrs', 'Miss':'Miss', 'Master':'Child', \n",
    "                    'Don':'Titled', 'Rev':'Titled', 'Ms':'Mrs', 'Mme':'Mrs', 'Mlle':'Miss',\n",
    "                    'Dr':'Titled', 'Lady':'Titled', 'Col':'Officer', 'the Countess':'Titled',\n",
    "                    'Jonkheer':'Titled', 'Major':'Officer', 'Capt':'Officer', 'Dona':'Titled', 'Sir':'Titled'\n",
    "                }\n",
    "    \n",
    "    return title_lookup[extract_from_name(name, part='Title')]\n",
    "\n",
    "\n",
    "def extract_from_name(name, part='LastName'):\n",
    "    '''Extracting last name or title from name'''\n",
    "    \n",
    "    part_lookup = {'LastName':0, 'Title':1}\n",
    "    \n",
    "    parts = name.split(\",\")\n",
    "#     parts[0] = parts[0][-3:]\n",
    "    parts[1] = parts[1].split(\".\")[0].strip()\n",
    "    \n",
    "    return parts[part_lookup[part]]\n",
    "\n",
    "\n",
    "def fill_in_median_val(passenger, column_name, measure_by_class):\n",
    "    '''Fill in a column name using the class, sex and title measures.'''\n",
    "    if np.isnan(passenger[column_name]):\n",
    "        #This should be changed to include class, sex and title too\n",
    "        meas = np.mean(\\\n",
    "                       measure_by_class[(measure_by_class[\"Sex\"] == passenger['Sex']) & \\\n",
    "                                        (measure_by_class[\"Pclass\"] == passenger['Pclass']) & \\\n",
    "                                        (measure_by_class[\"Title\"] == passenger[\"Title\"])\\\n",
    "                                       ][column_name])\n",
    "        if np.isnan(meas):\n",
    "            meas = np.mean(\\\n",
    "                           measure_by_class[\\\n",
    "                                            (measure_by_class[\"Sex\"] == passenger['Sex']) & \\\n",
    "                                            (measure_by_class[\"Pclass\"] == passenger['Pclass'])\\\n",
    "                                           ][column_name])\n",
    "        passenger[column_name] = meas\n",
    "    \n",
    "    return passenger\n",
    "\n",
    "\n",
    "def fill_in_fare(passenger, fare_by_class):\n",
    "    if np.isnan(passenger['Fare']):\n",
    "        passenger['Fare'] = fare_by_class[passenger['Pclass']]\n",
    "    \n",
    "    return passenger\n",
    "\n",
    "\n",
    "def get_family_bin(passenger):\n",
    "    if passenger['SibSp'] + passenger['ParCh'] + 1 > 3:\n",
    "        passenger['FamilySize'] = 1\n",
    "    else:\n",
    "        passenger['FamilySize'] = 0\n",
    "    \n",
    "    return passenger\n",
    "\n",
    "\n",
    "def get_is_mother(passenger):\n",
    "    if passenger['ParCh'] > 0 and passenger['Title'] == 'Mrs':\n",
    "        passenger['IsMother'] = 1\n",
    "    else:\n",
    "        passenger['IsMother'] = 0\n",
    "    \n",
    "    return passenger\n",
    "\n",
    "\n",
    "def get_is_orphan(passenger):\n",
    "    if passenger['ParCh'] == 0 and passenger['Age'] <= 8:\n",
    "        passenger['isOrphan'] = 1\n",
    "    else:\n",
    "        passenger['isOrphan'] = 0\n",
    "    \n",
    "    return passenger\n",
    "\n",
    "def ticket_has_known_survivor(passenger):\n",
    "    ticket = passenger[\"Ticket\"]\n",
    "    \n",
    "    train_df = get_original_df(TRAIN_FILE)\n",
    "    survivor_tickets = train_df.groupby(\"Ticket\")[\"Survived\"].sum()\n",
    "    total_tickets = train_df.groupby(\"Ticket\")[\"Survived\"].size()\n",
    "    \n",
    "    if ticket in survivor_tickets.keys():\n",
    "        passenger[\"KnownSurvivors\"] = survivor_tickets[ticket]/float(total_tickets[ticket])\n",
    "    else:\n",
    "        passenger[\"KnownSurvivors\"] = 0\n",
    "        \n",
    "    return passenger\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_data(df_train, df_test=None, mode='train'):\n",
    "    '''\n",
    "        This function featurises a raw dataset. \n",
    "        If a test set is provided in test mode, it combines it with\n",
    "        the training set and calculates the features accordingly.\n",
    "        \n",
    "    '''\n",
    "    df = df_train.copy()\n",
    "    \n",
    "    if mode == 'test':\n",
    "        df_test[\"Survived\"] = 2\n",
    "        df = pd.concat([df_train, df_test])\n",
    "    \n",
    "    ## Extract featuers from name\n",
    "    df[\"LastName\"] = df.Name.map(extract_from_name)\n",
    "    df[\"Title\"] = df.Name.map(get_title)\n",
    "    \n",
    "    \n",
    "    #Fill in missing age and fare by class using median values\n",
    "    age_by_class = df.groupby(['Sex','Pclass','Title'], as_index=False).Age.median()\n",
    "    fare_by_class = df.groupby(['Sex','Pclass','Title'], as_index=False).Fare.median()\n",
    "\n",
    "    df = df.apply(lambda passenger: fill_in_median_val(passenger,'Age', age_by_class), axis=1)\n",
    "    df = df.apply(lambda passenger: fill_in_median_val(passenger, 'Fare', fare_by_class), axis=1)\n",
    "    \n",
    "    \n",
    "#     Divide age by two (and make it int) to reduce sparsity\n",
    "#     df.Age = df.Age.map(lambda x: int(x)/2)\n",
    "    \n",
    "    # Calculate family size, adding 1 to include the person. 1 if above 3, else 0.\n",
    "    df[\"FamilySize\"] = 0\n",
    "    df = df.apply(get_family_bin, axis=1)\n",
    "    \n",
    "    # Calculate family size, adding 1 to include the person. 1 if above 3, else 0.\n",
    "#     df[\"Fare\"] = df.apply(lambda x: x[\"Fare\"]/(x['SibSp'] + x['ParCh'] + 1), axis=1)\n",
    "#     df = df.apply(get_family_bin, axis=1)\n",
    "    \n",
    "    # Check if person is mother\n",
    "    df[\"IsMother\"] = 0\n",
    "    df = df.apply(get_is_mother, axis=1) \n",
    "    \n",
    "#     Check if ticket has survivors\n",
    "#     df[\"KnownSurvivors\"] = 0\n",
    "#     df = df.apply(ticket_has_known_survivor, axis=1)\n",
    "    \n",
    "    # Create dummies for Sex\n",
    "    sex = pd.get_dummies(df['Sex'])\n",
    "    df = pd.concat([df, sex], axis=1)\n",
    "    \n",
    "    # Create dummies for Ticket. Didn't work out too well.\n",
    "#     Ticket = pd.get_dummies(df['Ticket'])\n",
    "#     df = pd.concat([df, Ticket], axis=1)\n",
    "    \n",
    "    # Create dummy variables for cabin after extracting first letter of cabin. Didn't work out too well.\n",
    "    df[\"Cabin\"] = df.Cabin.map(lambda x: \"N/A\" if x is np.nan else x[0])\n",
    "#     df.Cabin = df.Cabin.map(lambda x: x[0])\n",
    "#     Cabin = pd.get_dummies(df['Cabin'])\n",
    "#     df = pd.concat([df, Cabin], axis=1)\n",
    "\n",
    "    # Change cabin to indicate whether it is assigned or not, create dummies\n",
    "    df[\"Cabin\"] = df.Cabin.map(lambda x: \"NoCabin\" if x == \"N/A\" else \"HasCabin\")\n",
    "    Cabin = pd.get_dummies(df['Cabin'])\n",
    "    df = pd.concat([df, Cabin], axis=1)\n",
    "    \n",
    "    \n",
    "    # Create dummies for embarkation\n",
    "    embarkation = pd.get_dummies(df['Embarked'])\n",
    "    df = pd.concat([df, embarkation], axis=1)\n",
    "\n",
    "#     Create dummies based on family name. Didn't work out too well.\n",
    "#     LastName = pd.get_dummies(df['LastName'])\n",
    "#     df = pd.concat([df, LastName], axis=1)\n",
    "\n",
    "    # Create dummies based on title\n",
    "    Title = pd.get_dummies(df['Title'])\n",
    "    df = pd.concat([df, Title], axis=1)\n",
    "\n",
    "    # Drop the columns that have been onehot encoded\n",
    "    df.drop(['Sex', 'LastName',\n",
    "             'Embarked', 'Cabin', 'Title', 'PassengerID', 'Name', 'Ticket'], axis=1, inplace=True)\n",
    "    \n",
    "    #Drop training records if test set\n",
    "    if mode == 'test':\n",
    "        df = df[df.Survived == 2]\n",
    "        \n",
    "    # Drop survived if it is a training set\n",
    "    if 'Survived' in df.columns:\n",
    "        df.drop('Survived', axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions for creating, training, test sets and submitting files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_extra_features(X_test):\n",
    "    '''Function to remove extra columns from test set, and add missing columns based on training set'''\n",
    "    X, y = get_training_data()\n",
    "    \n",
    "    for col in X.columns:\n",
    "        if col not in X_test.columns:\n",
    "            X_test[col] = 0\n",
    "            \n",
    "    for col in X_test.columns:\n",
    "        if col not in X.columns:\n",
    "            X_test.drop([col], axis=1, inplace=True) \n",
    "    \n",
    "    return X_test\n",
    "\n",
    "def get_training_data():\n",
    "    '''Returns original training data'''\n",
    "    df_original = get_original_df(TRAIN_FILE)\n",
    "    X = featurize_data(df_original)\n",
    "    y = df_original['Survived'].copy()\n",
    "    X.fillna(0, inplace=True)\n",
    "    return X, y\n",
    "\n",
    "def get_test_data():\n",
    "    '''Returns original test data'''\n",
    "    df_test_original = get_original_df(TEST_FILE)\n",
    "    df_train_original = get_original_df(TRAIN_FILE)\n",
    "    \n",
    "    X_test = featurize_data(df_train_original, df_test_original, mode='test')\n",
    "    \n",
    "    X_test = clean_extra_features(X_test)\n",
    "    \n",
    "    X.fillna(0, inplace=True)\n",
    "    return X_test\n",
    "\n",
    "def get_sparse_test_data():\n",
    "    '''Random testing file'''\n",
    "    df_test_original = get_original_df(SPARSE_TEST_FILE)\n",
    "    df_train_original = get_original_df(TRAIN_FILE)\n",
    "    \n",
    "    X = featurize_data(df_test_original)\n",
    "    X = clean_extra_features(X_test=X)\n",
    "    y = df_test_original['Survived'].copy()\n",
    "    X.fillna(0, inplace=True)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training data and split it\n",
    "X, y = get_training_data()\n",
    "# X_sparse, y_sparse = get_sparse_test_data()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=424242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Classifiers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(pipeline, sub_msg, retrain=True):\n",
    "    '''This creates a submission file with or without retraining the model.'''\n",
    "    X, y = get_training_data()\n",
    "    X_test = get_test_data()\n",
    "    \n",
    "    df_test = get_original_df(TEST_FILE)\n",
    "    \n",
    "    if retrain:\n",
    "        pipeline.fit(X, y)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    submission_df = pd.concat([df_test.PassengerID, pd.DataFrame(y_pred)], axis=1)\n",
    "    submission_df.columns = ['PassengerID','Survived']\n",
    "    \n",
    "    joblib.dump(pipeline, '../models/model_{}.pkl'.format(sub_msg)) \n",
    "    \n",
    "    return submission_df.to_csv('../data/output/submission_{}.csv'.format(sub_msg), index=False)\n",
    "\n",
    "def get_sparse_accuracy(pipeline, sub_msg=\"\", retrain=True, repeat=1):\n",
    "    '''Test function, based on test data -- ignore'''\n",
    "    X, y = get_training_data()\n",
    "    X_sparse_test, y_sparse_test = get_sparse_test_data()\n",
    "    \n",
    "    accuracy_scores = []\n",
    "    for _ in range(repeat):\n",
    "        if retrain:\n",
    "            pipeline.fit(X, y)\n",
    "        y_pred = pipeline.predict(X_sparse_test)\n",
    "        accuracy_scores.append(accuracy_score(y_pred, y_sparse_test))\n",
    "    \n",
    "    return max(accuracy_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_models(clfs, X_train, y_train, X_test, y_test):\n",
    "    for clf in clfs:\n",
    "        print(\"=\"*80)\n",
    "        print(clf)\n",
    "        get_accuracy_clf(clf, X_train, y_train, X_test, y_test)\n",
    "        print()\n",
    "        \n",
    "\n",
    "def get_accuracy_clf(clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('accuracy: {:.3f}%, {:.3f}%'.format(accuracy*100, get_sparse_accuracy(clf, sub_msg=\"\")*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a pipeline and test\n",
    "scaler = StandardScaler()\n",
    "feature_select = SelectKBest(f_classif, k=20)\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=[\n",
    "                                    ('gbc', GradientBoostingClassifier()), \n",
    "                                    ('rr', RandomForestClassifier()), \n",
    "#                                     ('lr', LogisticRegression()),\n",
    "#                                     ('nn', MLPClassifier()),\n",
    "#                                     ('ab', AdaBoostClassifier()),\n",
    "#                                     ('bnb', BernoulliNB()),\n",
    "#                                     ('svc', SVC(kernel='linear',probability=True))\n",
    "                                    ], voting='soft', weights=[3,2])\n",
    "\n",
    "eclf2 = VotingClassifier(estimators=[\n",
    "                                    ('gbc', GradientBoostingClassifier(max_depth=4)), \n",
    "                                    ('rr', RandomForestClassifier(max_depth=5)), \n",
    "                                    ('lr', LogisticRegression()),\n",
    "#                                     ('nn', MLPClassifier())\n",
    "                                    ], voting='soft')\n",
    "\n",
    "pipeline1 = Pipeline([('scale', scaler), ('select_feat', feature_select), ('clf', eclf1)])\n",
    "pipeline2 = Pipeline([('scale', scaler), ('select_feat', feature_select), ('clf', eclf2)])\n",
    "\n",
    "clfs = [LogisticRegression(), GradientBoostingClassifier(), RandomForestClassifier(),eclf1, eclf2, pipeline1, pipeline2, AdaBoostClassifier(), MLPClassifier(), SVC(kernel='rbf',probability=True)]\n",
    "baseline_models(clfs, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_estimator(grid_search, X, y, is_pipeline=False):\n",
    "    '''Return best grid search parameters and print details'''\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print()\n",
    "    print(\"Best parameters set:\")\n",
    "    if is_pipeline:\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "    \n",
    "        for component in best_parameters:\n",
    "\n",
    "\n",
    "\n",
    "            if component == 'clf':\n",
    "                classifiers = best_parameters[component].get_params()\n",
    "                for part in classifiers:\n",
    "                    if '__' not in part:\n",
    "                        print(part + \":\")\n",
    "                        print(classifiers[part])\n",
    "                        print()\n",
    "                continue\n",
    "            if '__' not in component:\n",
    "                print(component)\n",
    "                print(best_parameters[component])\n",
    "                print()\n",
    "    #     print(best_parameters['clf'].get_params())\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get best classifier models for the algorithms in the list. \n",
    "## The commented out code is a reference for the parameters available.\n",
    "\n",
    "rr = (bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
    "            verbose=0, warm_start=False)\n",
    "\n",
    "gbm = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.1, loss='deviance', max_depth=4,\n",
    "              max_features=None, max_leaf_nodes=None,\n",
    "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "              n_estimators=100, presort='auto', random_state=None,\n",
    "              subsample=1.0, verbose=0, warm_start=False)\n",
    "\n",
    "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "\n",
    "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "       verbose=False, warm_start=False)\n",
    "accuracy: 79.235%, 71.066%\n",
    "\n",
    "# ================================================================================\n",
    "# SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "#   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "#   max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "#   tol=0.001, verbose=False)\n",
    "\n",
    "clf_rr = RandomForestClassifier()\n",
    "clf_lr = LogisticRegression()\n",
    "clf_gb = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "param_grid_rr = [{\n",
    "                    'bootstrap':[True, False],\n",
    "                    'class_weight':[None, 'balanced'],\n",
    "                    'max_features':[None, 'auto'],\n",
    "                    'n_estimators': [10, 20, 100, 200, 1000]\n",
    "        \n",
    "    }]\n",
    "\n",
    "param_grid_gb = [{\n",
    "                    'max_depth':[None, 2, 3, 4, 5],\n",
    "                    'max_features':[None, 'auto'],\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'learning_rate': [0.1, 0.01, 0.001]\n",
    "        \n",
    "    }]\n",
    "\n",
    "param_grid_lr = [{\n",
    "                    'class_weight':[None, 'balanced'],\n",
    "                    'C':[2**-5, 2**-3, 2**-1, 2**0, 2**2, 2**4, 2**6],\n",
    "                    'penalty': ['l1', 'l2'],\n",
    "        \n",
    "    }]\n",
    "\n",
    "# param_grid_svm = [{\n",
    "#                     'alpha':[0.01, 0.1, 0.0001],\n",
    "#                     'max_features':[None, 'auto'],\n",
    "#                     'n_estimators': [50, 100, 200],\n",
    "#                     'learning_rate': [0.1, 0.001]\n",
    "        \n",
    "#     }]\n",
    "\n",
    "# param_grid_nn = [{\n",
    "#                     'class_weight':[None, 'balanced'],\n",
    "#                     'C':[2**-5, 2**-3, 2**-1, 2**0, 2**2, 2**4, 2**6],\n",
    "#                     'penalty': ['l1', 'l2'],\n",
    "        \n",
    "#     }]\n",
    "\n",
    "clfs = {clf_rr:param_grid_rr, clf_lr:param_grid_lr, clf_gb:param_grid_gb}\n",
    "\n",
    "for clf in clfs.keys():\n",
    "#     print(clf)\n",
    "    print(get_sparse_accuracy(clf, repeat=10))\n",
    "    grid_search = GridSearchCV(clf, clfs[clf], n_jobs=- 1, verbose=1, cv=10)\n",
    "    clf = get_best_estimator(grid_search, X, y)\n",
    "    print(get_sparse_accuracy(clf, repeat=10))\n",
    "#     print(clf)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging - Voting Classifier\n",
    "\n",
    "Warning - Run the grid search at your own risk. This usually is run with multiple parameters because our machine can handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is to set up the parameter grid for the pipeline to enter cross validation and grid search\n",
    "\n",
    "\n",
    "param_grid = [{\n",
    "#                 'clf__lr__penalty':['l1', 'l2'],\n",
    "                'clf__lr__C':[2**0, 2**0.5, 2**1, 2**5],\n",
    "                'clf__lr__class_weight':['balanced', None],\n",
    "#                 'clf__lr__penalty':['l1', 'l2'],\n",
    "#         'clf__rr__criterion': ['gini','entropy'],\n",
    "        'clf__rr__class_weight':[None, 'balanced'],\n",
    "        'clf__rr__max_depth':[3,4, None],\n",
    "#         'clf__rr__max_features':[15, 'auto'],\n",
    "        'clf__rr__n_estimators':[10, 20, 50, 100, 200],\n",
    "#         'clf__nn__activation': ['relu'],\n",
    "#         'clf__nn__alpha':[0.001, 0.1],\n",
    "#         'clf__nn__solver':['adam'],\n",
    "#         'clf__nn__max_iter':[500],\n",
    "#                 'clf__gbc__learning_rate':[0.1, 0.001],\n",
    "                'clf__gbc__n_estimators':[100, 200, 500],\n",
    "#                 'clf__gbc__max_features':['auto', None],\n",
    "#                 'clf__gbc__warm_start':[True, False],\n",
    "                'clf__voting':['soft','hard']\n",
    "    }]\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, n_jobs=-1, verbose=1, cv=5)\n",
    "\n",
    "pipeline = get_best_estimator(grid_search, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as the best estimator was found, a submission file was created without retraining the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission_file(pipeline, sub_msg=\"best_estimator\", retrain=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X=X, y=y)\n",
    "features = pd.DataFrame()\n",
    "features['feature'] = X.columns\n",
    "features['importance'] = clf.feature_importances_\n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "train_new = model.transform(X)\n",
    "train_new.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.sort(['importance'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending and Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blended_prediction_model(clfs, X, y):\n",
    "    '''Train a list of classifiers and train a meta classifier on them'''\n",
    "    train_predictions_a_df = pd.DataFrame()\n",
    "    train_predictions_b_df = pd.DataFrame()\n",
    "    test_predictions_df = pd.DataFrame()\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=424242)\n",
    "    X_a_train, X_b_train, y_a_train, y_b_train = train_test_split(X_train, y_train, test_size=0.50, random_state=424242)\n",
    "    \n",
    "    for clf in clfs:\n",
    "        clf.fit(X_a_train, y_a_train)\n",
    "        y_predict_prob = clf.predict_proba(X_b_train)\n",
    "        y_predict = clf.predict(X_b_train)\n",
    "        train_predictions_a_df[str(clf).split(\"(\")[0] + \"_proba\"] = pd.Series(y_predict_prob[:,0])\n",
    "        train_predictions_a_df[str(clf).split(\"(\")[0] + \"_prediction\"] = y_predict\n",
    "        \n",
    "        clf.fit(X_b_train, y_b_train)\n",
    "        y_predict_prob = clf.predict_proba(X_a_train)\n",
    "        y_predict = clf.predict(X_a_train)\n",
    "        train_predictions_b_df[str(clf).split(\"(\")[0] + \"_proba\"] = pd.Series(y_predict_prob[:,0])\n",
    "        train_predictions_b_df[str(clf).split(\"(\")[0] + \"_prediction\"] = y_predict\n",
    "        \n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_predict_prob = clf.predict_proba(X_val)\n",
    "        y_predict = clf.predict(X_val)\n",
    "        \n",
    "        test_predictions_df[str(clf).split(\"(\")[0] + \"_proba\"] = pd.Series(y_predict_prob[:,0])\n",
    "        test_predictions_df[str(clf).split(\"(\")[0] + \"_prediction\"] = y_predict\n",
    "    \n",
    "    meta_model = LogisticRegression()\n",
    "    \n",
    "    meta_model.fit(train_predictions_a_df, y_b_train)\n",
    "    meta_model.fit(train_predictions_b_df, y_a_train)\n",
    "    y_pred = meta_model.predict(test_predictions_df)\n",
    "    \n",
    "    print(\"Meta Accuracy: {:.3f}%\".format(accuracy_score(y_pred, y_val) * 100))\n",
    "    \n",
    "    return meta_model\n",
    "\n",
    "def create_meta_pred_set(clfs, X, y, X_test):\n",
    "    '''Get data for metamodel training'''\n",
    "    predictions_df = pd.DataFrame()\n",
    "    for clf in clfs:\n",
    "        clf.fit(X, y)\n",
    "        y_predict_prob = clf.predict_proba(X_test)\n",
    "        y_predict = clf.predict(X_test)\n",
    "        predictions_df[str(clf).split(\"(\")[0] + \"_proba\"] = pd.Series(y_predict_prob[:,0])\n",
    "        predictions_df[str(clf).split(\"(\")[0] + \"_prediction\"] = y_predict\n",
    "    return predictions_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_training_data()\n",
    "X_test, y_test = get_sparse_test_data()\n",
    "\n",
    "bl_clf = get_blended_prediction_model(clfs, X, y)\n",
    "\n",
    "X_meta_test = create_meta_pred_set(clfs, X, y, X_test)\n",
    "y_pred = bl_clf.predict(X_meta_test)\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacked_prediction_model(clfs, X, y, k=10):\n",
    "    '''\n",
    "        This trains a list of classifiers in cross validated \n",
    "        manner and then trains another meta classifier based on\n",
    "        the predictions and probabilities of the original classifiers\n",
    "    '''\n",
    "    kf = KFold(n_splits=k, shuffle=False)\n",
    "\n",
    "    train_predictions = []\n",
    "    y_list = []\n",
    "    X = X.as_matrix()\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        train_k_predictions_df = pd.DataFrame()\n",
    "        for clf in clfs:\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_predict_prob = clf.predict_proba(X_test)\n",
    "            y_predict = clf.predict(X_test)\n",
    "            train_k_predictions_df[str(clf).split(\"(\")[0] + \"_proba\"] = pd.Series(y_predict_prob[:,0])\n",
    "            train_k_predictions_df[str(clf).split(\"(\")[0] + \"_prediction\"] = y_predict\n",
    "        y_list.append(y_test)\n",
    "        train_predictions.append(train_k_predictions_df)\n",
    "    \n",
    "    train_predictions_df = pd.concat(train_predictions)\n",
    "    \n",
    "    y = pd.concat(y_list) \n",
    "    X = train_predictions_df.as_matrix()\n",
    "    \n",
    "    stacked_model = LogisticRegression()\n",
    "    \n",
    "    stacked_model.fit(X, y)\n",
    "    y_predict = stacked_model.predict(X)\n",
    "    print(accuracy_score(y_predict, y))\n",
    "    \n",
    "    return stacked_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [\n",
    "#         LogisticRegression(), \n",
    "        GradientBoostingClassifier(), \n",
    "        RandomForestClassifier(),\n",
    "        eclf1, \n",
    "#         eclf2, \n",
    "        pipeline1, \n",
    "#         pipeline2, \n",
    "#         AdaBoostClassifier(), \n",
    "#         MLPClassifier(), \n",
    "#         SVC(kernel='rbf',probability=True)\n",
    "       ]\n",
    "\n",
    "X, y = get_training_data()\n",
    "X_test, y_test = get_sparse_test_data()\n",
    "\n",
    "acc_scores = []\n",
    "\n",
    "## Loop to see if if \n",
    "stacked_clf = get_stacked_prediction_model(clfs, X, y)\n",
    "\n",
    "X_meta_test = create_meta_pred_set(clfs, X, y, X_test)\n",
    "y_pred = stacked_clf.predict(X_meta_test)\n",
    "    \n",
    "score = accuracy_score(y_pred, y_test)\n",
    "acc_scores.append(accuracy_score(y_pred, y_test))\n",
    "    \n",
    "\n",
    "df_test = get_original_df(TEST_FILE)\n",
    "submission_df = pd.concat([df_test.PassengerID, pd.DataFrame(y_pred)], axis=1)\n",
    "submission_df.columns = ['PassengerID','Survived']\n",
    "\n",
    "sub_msg = \"trial\" + str(score)\n",
    "    \n",
    "joblib.dump((stacked_clf, clfs) , '../models/model_{}.pkl'.format(sub_msg)) \n",
    "submission_df.to_csv('../data/output/submission_{}.csv'.format(sub_msg), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(acc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = joblib.load('../models/model_best.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.named_steps['clf'].estimators[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
