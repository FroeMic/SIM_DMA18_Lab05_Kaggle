{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) #to supress import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COLUMNS = ['station','date','feature', 'value', 'measurement','quality', 'source', 'hour']\n",
    "COLUMNS_test = ['station','date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train = pd.read_csv('../data/export_features_loc_MAX.csv', index_col=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>date</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>elev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>128</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>145</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>140</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>162</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-09</td>\n",
       "      <td>115</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       station       date  TMIN     lat    long  elev\n",
       "0  AE000041196 2014-01-01   128  25.333  55.517  34.0\n",
       "1  AE000041196 2014-01-02   145  25.333  55.517  34.0\n",
       "2  AE000041196 2014-01-03   140  25.333  55.517  34.0\n",
       "3  AE000041196 2014-01-06   162  25.333  55.517  34.0\n",
       "4  AE000041196 2014-01-09   115  25.333  55.517  34.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['date'] = pd.to_datetime(df_train['date'], format='%Y%m%d', errors='ignore')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you want to use past days as predictor?\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>date</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>elev</th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>128</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>145</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>140</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-06</td>\n",
       "      <td>162</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-09</td>\n",
       "      <td>115</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-11</td>\n",
       "      <td>159</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-13</td>\n",
       "      <td>145</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-15</td>\n",
       "      <td>137</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-17</td>\n",
       "      <td>118</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-18</td>\n",
       "      <td>117</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-21</td>\n",
       "      <td>113</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-22</td>\n",
       "      <td>119</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-23</td>\n",
       "      <td>135</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-24</td>\n",
       "      <td>120</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-26</td>\n",
       "      <td>115</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-27</td>\n",
       "      <td>123</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-28</td>\n",
       "      <td>138</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-29</td>\n",
       "      <td>130</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-30</td>\n",
       "      <td>149</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-01-31</td>\n",
       "      <td>147</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-02-02</td>\n",
       "      <td>155</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-02-03</td>\n",
       "      <td>155</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-02-07</td>\n",
       "      <td>99</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-02-08</td>\n",
       "      <td>112</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-02-09</td>\n",
       "      <td>123</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-02-11</td>\n",
       "      <td>115</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-02-14</td>\n",
       "      <td>118</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-02-15</td>\n",
       "      <td>104</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-02-17</td>\n",
       "      <td>146</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-02-18</td>\n",
       "      <td>177</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-04-24</td>\n",
       "      <td>235</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-04-26</td>\n",
       "      <td>200</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-04-27</td>\n",
       "      <td>213</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-04-28</td>\n",
       "      <td>209</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-04-29</td>\n",
       "      <td>230</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-01</td>\n",
       "      <td>259</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-02</td>\n",
       "      <td>257</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-03</td>\n",
       "      <td>265</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>235</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>217</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-08</td>\n",
       "      <td>214</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-09</td>\n",
       "      <td>215</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-10</td>\n",
       "      <td>239</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-11</td>\n",
       "      <td>265</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-12</td>\n",
       "      <td>273</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-13</td>\n",
       "      <td>266</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-15</td>\n",
       "      <td>257</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-16</td>\n",
       "      <td>247</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-17</td>\n",
       "      <td>226</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-18</td>\n",
       "      <td>227</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-19</td>\n",
       "      <td>237</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-20</td>\n",
       "      <td>237</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-21</td>\n",
       "      <td>259</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-22</td>\n",
       "      <td>254</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-24</td>\n",
       "      <td>286</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-25</td>\n",
       "      <td>279</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-26</td>\n",
       "      <td>268</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-27</td>\n",
       "      <td>288</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-28</td>\n",
       "      <td>277</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>2014-05-29</td>\n",
       "      <td>286</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        station       date  TMIN     lat    long  elev  year  day\n",
       "0   AE000041196 2014-01-01   128  25.333  55.517  34.0  2014    1\n",
       "1   AE000041196 2014-01-02   145  25.333  55.517  34.0  2014    2\n",
       "2   AE000041196 2014-01-03   140  25.333  55.517  34.0  2014    3\n",
       "3   AE000041196 2014-01-06   162  25.333  55.517  34.0  2014    6\n",
       "4   AE000041196 2014-01-09   115  25.333  55.517  34.0  2014    9\n",
       "5   AE000041196 2014-01-11   159  25.333  55.517  34.0  2014   11\n",
       "6   AE000041196 2014-01-13   145  25.333  55.517  34.0  2014   13\n",
       "7   AE000041196 2014-01-15   137  25.333  55.517  34.0  2014   15\n",
       "8   AE000041196 2014-01-17   118  25.333  55.517  34.0  2014   17\n",
       "9   AE000041196 2014-01-18   117  25.333  55.517  34.0  2014   18\n",
       "10  AE000041196 2014-01-21   113  25.333  55.517  34.0  2014   21\n",
       "11  AE000041196 2014-01-22   119  25.333  55.517  34.0  2014   22\n",
       "12  AE000041196 2014-01-23   135  25.333  55.517  34.0  2014   23\n",
       "13  AE000041196 2014-01-24   120  25.333  55.517  34.0  2014   24\n",
       "14  AE000041196 2014-01-26   115  25.333  55.517  34.0  2014   26\n",
       "15  AE000041196 2014-01-27   123  25.333  55.517  34.0  2014   27\n",
       "16  AE000041196 2014-01-28   138  25.333  55.517  34.0  2014   28\n",
       "17  AE000041196 2014-01-29   130  25.333  55.517  34.0  2014   29\n",
       "18  AE000041196 2014-01-30   149  25.333  55.517  34.0  2014   30\n",
       "19  AE000041196 2014-01-31   147  25.333  55.517  34.0  2014   31\n",
       "20  AE000041196 2014-02-02   155  25.333  55.517  34.0  2014   33\n",
       "21  AE000041196 2014-02-03   155  25.333  55.517  34.0  2014   34\n",
       "22  AE000041196 2014-02-07    99  25.333  55.517  34.0  2014   38\n",
       "23  AE000041196 2014-02-08   112  25.333  55.517  34.0  2014   39\n",
       "24  AE000041196 2014-02-09   123  25.333  55.517  34.0  2014   40\n",
       "25  AE000041196 2014-02-11   115  25.333  55.517  34.0  2014   42\n",
       "26  AE000041196 2014-02-14   118  25.333  55.517  34.0  2014   45\n",
       "27  AE000041196 2014-02-15   104  25.333  55.517  34.0  2014   46\n",
       "28  AE000041196 2014-02-17   146  25.333  55.517  34.0  2014   48\n",
       "29  AE000041196 2014-02-18   177  25.333  55.517  34.0  2014   49\n",
       "..          ...        ...   ...     ...     ...   ...   ...  ...\n",
       "70  AE000041196 2014-04-24   235  25.333  55.517  34.0  2014  114\n",
       "71  AE000041196 2014-04-26   200  25.333  55.517  34.0  2014  116\n",
       "72  AE000041196 2014-04-27   213  25.333  55.517  34.0  2014  117\n",
       "73  AE000041196 2014-04-28   209  25.333  55.517  34.0  2014  118\n",
       "74  AE000041196 2014-04-29   230  25.333  55.517  34.0  2014  119\n",
       "75  AE000041196 2014-05-01   259  25.333  55.517  34.0  2014  121\n",
       "76  AE000041196 2014-05-02   257  25.333  55.517  34.0  2014  122\n",
       "77  AE000041196 2014-05-03   265  25.333  55.517  34.0  2014  123\n",
       "78  AE000041196 2014-05-05   235  25.333  55.517  34.0  2014  125\n",
       "79  AE000041196 2014-05-07   217  25.333  55.517  34.0  2014  127\n",
       "80  AE000041196 2014-05-08   214  25.333  55.517  34.0  2014  128\n",
       "81  AE000041196 2014-05-09   215  25.333  55.517  34.0  2014  129\n",
       "82  AE000041196 2014-05-10   239  25.333  55.517  34.0  2014  130\n",
       "83  AE000041196 2014-05-11   265  25.333  55.517  34.0  2014  131\n",
       "84  AE000041196 2014-05-12   273  25.333  55.517  34.0  2014  132\n",
       "85  AE000041196 2014-05-13   266  25.333  55.517  34.0  2014  133\n",
       "86  AE000041196 2014-05-15   257  25.333  55.517  34.0  2014  135\n",
       "87  AE000041196 2014-05-16   247  25.333  55.517  34.0  2014  136\n",
       "88  AE000041196 2014-05-17   226  25.333  55.517  34.0  2014  137\n",
       "89  AE000041196 2014-05-18   227  25.333  55.517  34.0  2014  138\n",
       "90  AE000041196 2014-05-19   237  25.333  55.517  34.0  2014  139\n",
       "91  AE000041196 2014-05-20   237  25.333  55.517  34.0  2014  140\n",
       "92  AE000041196 2014-05-21   259  25.333  55.517  34.0  2014  141\n",
       "93  AE000041196 2014-05-22   254  25.333  55.517  34.0  2014  142\n",
       "94  AE000041196 2014-05-24   286  25.333  55.517  34.0  2014  144\n",
       "95  AE000041196 2014-05-25   279  25.333  55.517  34.0  2014  145\n",
       "96  AE000041196 2014-05-26   268  25.333  55.517  34.0  2014  146\n",
       "97  AE000041196 2014-05-27   288  25.333  55.517  34.0  2014  147\n",
       "98  AE000041196 2014-05-28   277  25.333  55.517  34.0  2014  148\n",
       "99  AE000041196 2014-05-29   286  25.333  55.517  34.0  2014  149\n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get year and date as features\n",
    "df = df_train\n",
    "df = df_train_yd.drop(columns='date')\n",
    "df['year'] = df_train['date'].map(lambda x: x.year)\n",
    "df['day'] = df_train['date'].map(lambda x: x.timetuple().tm_yday)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to reduce preproccesing duration\n",
    "#df_train_yd.to_csv('../data/tmp/export_LSMT_MAX_yd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data ready for LMST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the preprocessed data for a quicker start\n",
    "df = pd.read_csv('../data/tmp/export_LSMT_MAX_yd.csv', index_col=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>elev</th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4039</th>\n",
       "      <td>AG000060590</td>\n",
       "      <td>30</td>\n",
       "      <td>30.5667</td>\n",
       "      <td>2.8667</td>\n",
       "      <td>397.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4040</th>\n",
       "      <td>AG000060590</td>\n",
       "      <td>31</td>\n",
       "      <td>30.5667</td>\n",
       "      <td>2.8667</td>\n",
       "      <td>397.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>AG000060590</td>\n",
       "      <td>36</td>\n",
       "      <td>30.5667</td>\n",
       "      <td>2.8667</td>\n",
       "      <td>397.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4042</th>\n",
       "      <td>AG000060590</td>\n",
       "      <td>60</td>\n",
       "      <td>30.5667</td>\n",
       "      <td>2.8667</td>\n",
       "      <td>397.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4043</th>\n",
       "      <td>AG000060590</td>\n",
       "      <td>50</td>\n",
       "      <td>30.5667</td>\n",
       "      <td>2.8667</td>\n",
       "      <td>397.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          station  TMIN      lat    long   elev  year  day\n",
       "4039  AG000060590    30  30.5667  2.8667  397.0  2014    1\n",
       "4040  AG000060590    31  30.5667  2.8667  397.0  2014    2\n",
       "4041  AG000060590    36  30.5667  2.8667  397.0  2014    3\n",
       "4042  AG000060590    60  30.5667  2.8667  397.0  2014    4\n",
       "4043  AG000060590    50  30.5667  2.8667  397.0  2014    5"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick random stations for test and training\n",
    "seed = 93598357\n",
    "np.random.seed(seed)\n",
    "stations = df.station.unique()\n",
    "np.random.shuffle(stations)\n",
    "stations_shuffled = stations\n",
    "stations_train = stations_shuffled[:2000]\n",
    "stations_holdout = stations_shuffled[2000:4000]\n",
    "\n",
    "df_train_test = df[df['station'].isin(stations_train)]\n",
    "df_train_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1792641, 7) (514252, 7)\n"
     ]
    }
   ],
   "source": [
    "#divide test and training to test effective of model to different timeframe (start of 2017)\n",
    "training_years = [2014,2015,2016]\n",
    "testing_days = range(80)\n",
    "\n",
    "df_train = df_train_test[df_train_test['year'].isin(training_years)]\n",
    "df_test = df_train_test[df_train_test['day'].isin(testing_days)]\n",
    "print(df_train.shape,df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate target from features\n",
    "df_X_train_raw = df_train.drop(columns='TMIN')\n",
    "df_X_test_raw = df_test.drop(columns='TMIN')\n",
    "sy_train = df_train['TMIN'].values\n",
    "sy_test = df_test['TMIN'].values\n",
    "y_train_raw = sy_train.reshape(-1,1)\n",
    "y_test_raw = sy_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1792641, 6)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_train_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int encode stations\n",
    "#LB = LabelBinarizer()\n",
    "#df_X['station'] = LB.fit_transform(df_X[['station']])\n",
    "df_X_train_red = df_X_train_raw.drop(columns='station')\n",
    "df_X_test_red = df_X_test_raw.drop(columns='station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_dict = df_X.to_dict('records')\n",
    "#vec = DictVectorizer()\n",
    "#X = vec.fit_transform(X_dict).toarray()\n",
    "#X_dummies = pd.get_dummies(df_X)\n",
    "#X = X_dummies.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features\n",
    "X_train_raw = df_X_train_red.values\n",
    "X_test_raw = df_X_test_red.values\n",
    "X_train_raw = X_train_raw.astype('float32')\n",
    "X_test_raw = X_test_raw.astype('float32')\n",
    "y_train_raw = y_train_raw.astype('float32')\n",
    "y_test_raw = y_test_raw.astype('float32')\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))                             \n",
    "train_X = scaler.fit_transform(X_train_raw)\n",
    "test_X = scaler.fit_transform(X_test_raw)\n",
    "train_y = scaler.fit_transform(y_train_raw)\n",
    "test_y = scaler.fit_transform(y_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1792641, 5) (1792641, 1) (514252, 5) (514252, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape,train_y.shape,test_X.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the number of lag days\n",
    "n_days = 0\n",
    "n_features = 1\n",
    "# frame as supervised learning\n",
    "#reframed = scaled #series_to_supervised(scaled, n_days, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((1792641,1,5))\n",
    "test_X = test_X.reshape((514252,1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1792641 samples, validate on 514252 samples\n",
      "Epoch 1/20\n",
      " - 50s - loss: 0.0273 - val_loss: 0.1103\n",
      "Epoch 2/20\n",
      " - 46s - loss: 0.0199 - val_loss: 0.0960\n",
      "Epoch 3/20\n",
      " - 46s - loss: 0.0195 - val_loss: 0.0828\n",
      "Epoch 4/20\n",
      " - 37s - loss: 0.0194 - val_loss: 0.0742\n",
      "Epoch 5/20\n",
      " - 36s - loss: 0.0195 - val_loss: 0.0689\n",
      "Epoch 6/20\n",
      " - 37s - loss: 0.0195 - val_loss: 0.0655\n",
      "Epoch 7/20\n",
      " - 37s - loss: 0.0196 - val_loss: 0.0631\n",
      "Epoch 8/20\n",
      " - 37s - loss: 0.0196 - val_loss: 0.0612\n",
      "Epoch 9/20\n",
      " - 41s - loss: 0.0196 - val_loss: 0.0597\n",
      "Epoch 10/20\n",
      " - 47s - loss: 0.0196 - val_loss: 0.0585\n",
      "Epoch 11/20\n",
      " - 44s - loss: 0.0197 - val_loss: 0.0575\n",
      "Epoch 12/20\n",
      " - 47s - loss: 0.0197 - val_loss: 0.0567\n",
      "Epoch 13/20\n",
      " - 49s - loss: 0.0197 - val_loss: 0.0560\n",
      "Epoch 14/20\n",
      " - 50s - loss: 0.0197 - val_loss: 0.0555\n",
      "Epoch 15/20\n",
      " - 49s - loss: 0.0198 - val_loss: 0.0550\n",
      "Epoch 16/20\n",
      " - 43s - loss: 0.0197 - val_loss: 0.0546\n",
      "Epoch 17/20\n",
      " - 50s - loss: 0.0197 - val_loss: 0.0542\n",
      "Epoch 18/20\n",
      " - 49s - loss: 0.0196 - val_loss: 0.0538\n",
      "Epoch 19/20\n",
      " - 47s - loss: 0.0196 - val_loss: 0.0533\n",
      "Epoch 20/20\n",
      " - 47s - loss: 0.0194 - val_loss: 0.0530\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15ebb2be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# design network\n",
    "learning_rate = 0.2\n",
    "epochs = 20\n",
    "decay_rate = learning_rate / epochs\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1],train_X.shape[2])))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(1))\n",
    "adam = Adam(lr = learning_rate, decay=decay_rate)\n",
    "model.compile(loss='mae', optimizer=adam)\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=epochs, batch_size=200, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 162.513\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = np.concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = np.concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for optimal parameters\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(train_X.shape[1],train_X.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "np.random.seed(seed)\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 20, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(train_X, train_y).score(X_test, y_test)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected lstm_15_input to have 3 dimensions, but got array with shape (514252, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-326ac8db1067>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# make a prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# invert scaling for forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minv_yhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/WebDev/DataMining/.dma/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1025\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/WebDev/DataMining/.dma/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1823\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1825\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/WebDev/DataMining/.dma/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected lstm_15_input to have 3 dimensions, but got array with shape (514252, 5)"
     ]
    }
   ],
   "source": [
    "# grid search for optimal parameters\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(train_X.shape[1],train_X.shape[2])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "np.random.seed(seed)\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = \n",
    "epochs = \n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(dropout_rate=dropout_rate)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(train_X, train_y).score(X_test, y_test)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = np.concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = np.concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kaggle scoring data\n",
    "df_score = pd.read_csv('../data/2018_test.csv', header=None, names=COLUMNS_test, low_memory=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
