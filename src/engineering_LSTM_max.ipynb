{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) #to supress import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data ready for LMST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the preprocessed data for a quicker start\n",
    "df = pd.read_csv('export_LSMT_MAX_yd.csv', index_col=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the preprocessed data for a quicker start\n",
    "\n",
    "TRAIN_FILE = '../data/tmp/export_LSMT_MAX_yd.csv'\n",
    "STATIONS_FILE = '../data/ghcnd-stations.csv'\n",
    "\n",
    "def get_original_df():\n",
    "    df_original = pd.read_csv(TRAIN_FILE)\n",
    "    df_original = df_original.drop(['Unnamed: 0'], axis=1)\n",
    "    return df_original\n",
    "\n",
    "def get_station_df():\n",
    "     return pd.read_csv(STATIONS_FILE, header=None, names=['station','lat', 'long', 'elev'], sep=';')\n",
    "\n",
    "def add_coordinates(df_src, df_stations, src_index='station', foreign_index='station'):\n",
    "    df_out = df_src.copy()\n",
    "    return df_out.join(df_stations.set_index(foreign_index), on=src_index)\n",
    "\n",
    "\n",
    "def add_day_of_year_column(df_src, column_name='date'):\n",
    "    df_out = df_src.copy()\n",
    "    df_out['day'] = df_out[column_name].apply(lambda d: date_to_nth_day(str(d)))\n",
    "    return df_out\n",
    "\n",
    "def date_to_nth_day(date, format='%Y%m%d'):\n",
    "    date = datetime.strptime(date, format)\n",
    "    new_year_day = datetime(year=date.year, month=1, day=1)\n",
    "    return (date - new_year_day).days + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick random stations for test and training\n",
    "seed = 93598357\n",
    "np.random.seed(seed)\n",
    "stations = df.station.unique()\n",
    "np.random.shuffle(stations)\n",
    "stations_shuffled = stations\n",
    "fraction = 64\n",
    "stations_train = stations_shuffled[:int(np.round(len(stations)/fraction))]\n",
    "stations_holdout14 = stations_shuffled[int(np.round(len(stations)/fraction)):int(np.round(len(stations)/fraction*2))]\n",
    "stations_holdout15 = stations_shuffled[int(np.round(len(stations)/fraction*2)):int(np.round(len(stations)/fraction*3))]\n",
    "stations_holdout16 = stations_shuffled[int(np.round(len(stations)/fraction*3)):int(np.round(len(stations)/fraction*4))]\n",
    "\n",
    "df_17 = df[df['station'].isin(stations_train)]\n",
    "df_14 = df[df['station'].isin(stations_holdout14)]\n",
    "df_15 = df[df['station'].isin(stations_holdout15)]\n",
    "df_16 = df[df['station'].isin(stations_holdout16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288148 285757 280102 287220\n"
     ]
    }
   ],
   "source": [
    "print(len(df_17), len(df_14), len(df_15), len(df_16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217112, 7) (17658, 7)\n",
      "(211770, 7) (18086, 7)\n",
      "(207350, 7) (17889, 7)\n",
      "(215511, 7) (17361, 7)\n"
     ]
    }
   ],
   "source": [
    "#divide test and training to test effective of model to different timeframe (start of 2017)\n",
    "training_years = [2014,2015,2016]\n",
    "testing_days = list(range(90))\n",
    "\n",
    "df_train17 = df_17[df_17['year'].isin(training_years)]\n",
    "df_test17 = df_17[~df_17['year'].isin(training_years)]\n",
    "df_test17 = df_test17[df_test17['day'].isin(testing_days)]\n",
    "print(df_train17.shape,df_test17.shape)\n",
    "\n",
    "training_years = [2017,2015,2016]\n",
    "df_train14 = df_14[df_14['year'].isin(training_years)]\n",
    "df_test14 = df_14[~df_14['year'].isin(training_years)]\n",
    "df_test14 = df_test14[df_test14['day'].isin(testing_days)]\n",
    "print(df_train14.shape,df_test14.shape)\n",
    "\n",
    "training_years = [2017,2014,2016]\n",
    "df_train15 = df_15[df_15['year'].isin(training_years)]\n",
    "df_test15 = df_15[~df_15['year'].isin(training_years)]\n",
    "df_test15 = df_test15[df_test15['day'].isin(testing_days)]\n",
    "print(df_train15.shape,df_test15.shape)\n",
    "\n",
    "training_years = [2017,2015,2014]\n",
    "df_train16 = df_16[df_16['year'].isin(training_years)]\n",
    "df_test16 = df_16[~df_16['year'].isin(training_years)]\n",
    "df_test16 = df_test16[df_test16['day'].isin(testing_days)]\n",
    "print(df_train16.shape,df_test16.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define split for CV later on\n",
    "split = [[df_train17.index.values, df_test17.index.values], [df_train16.index.values, df_test16.index.values],\n",
    "         [df_train15.index.values, df_test15.index.values],[df_train14.index.values, df_test14.index.values]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate target from features\n",
    "df_X_raw = df.drop(columns='TMIN')\n",
    "sy = df['TMIN']\n",
    "y_raw = sy.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18683824, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int encode stations\n",
    "#LB = LabelBinarizer()\n",
    "#df_X['station'] = LB.fit_transform(df_X[['station']])\n",
    "df_X_red = df_X_raw.drop(columns='station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_dict = df_X.to_dict('records')\n",
    "#vec = DictVectorizer()\n",
    "#X = vec.fit_transform(X_dict).toarray()\n",
    "#X_dummies = pd.get_dummies(df_X)\n",
    "#X = X_dummies.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features\n",
    "X_raw = df_X_red.values\n",
    "y_raw = y_raw.astype('float32')\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))                             \n",
    "X = scaler.fit_transform(X_raw)\n",
    "y = scaler.fit_transform(y_raw).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "X = X.reshape((len(X),1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
      "[CV] learning_rate=0.01, epochs=20, dropout_rate=0.4, batch_size=500 .\n",
      "[CV] learning_rate=0.01, epochs=20, dropout_rate=0.4, batch_size=500 .\n",
      "[CV] learning_rate=0.01, epochs=20, dropout_rate=0.4, batch_size=500 .\n",
      "[CV] learning_rate=0.01, epochs=20, dropout_rate=0.4, batch_size=500 .\n",
      "[CV]  learning_rate=0.01, epochs=20, dropout_rate=0.4, batch_size=500, total= 1.9min\n",
      "[CV] learning_rate=0.1, epochs=50, dropout_rate=0.8, batch_size=100 ..\n",
      "[CV]  learning_rate=0.01, epochs=20, dropout_rate=0.4, batch_size=500, total= 1.9min\n",
      "[CV]  learning_rate=0.01, epochs=20, dropout_rate=0.4, batch_size=500, total= 2.0min\n",
      "[CV] learning_rate=0.1, epochs=50, dropout_rate=0.8, batch_size=100 ..\n",
      "[CV] learning_rate=0.1, epochs=50, dropout_rate=0.8, batch_size=100 ..\n",
      "[CV]  learning_rate=0.01, epochs=20, dropout_rate=0.4, batch_size=500, total= 2.1min\n",
      "[CV] learning_rate=0.1, epochs=50, dropout_rate=0.8, batch_size=100 ..\n",
      "[CV]  learning_rate=0.1, epochs=50, dropout_rate=0.8, batch_size=100, total=11.7min\n",
      "[CV] learning_rate=0.05, epochs=20, dropout_rate=0.6, batch_size=100 .\n",
      "[CV]  learning_rate=0.1, epochs=50, dropout_rate=0.8, batch_size=100, total=12.0min\n",
      "[CV] learning_rate=0.05, epochs=20, dropout_rate=0.6, batch_size=100 .\n",
      "[CV]  learning_rate=0.1, epochs=50, dropout_rate=0.8, batch_size=100, total=12.1min\n",
      "[CV] learning_rate=0.05, epochs=20, dropout_rate=0.6, batch_size=100 .\n",
      "[CV]  learning_rate=0.1, epochs=50, dropout_rate=0.8, batch_size=100, total=12.0min\n",
      "[CV] learning_rate=0.05, epochs=20, dropout_rate=0.6, batch_size=100 .\n",
      "[CV]  learning_rate=0.05, epochs=20, dropout_rate=0.6, batch_size=100, total= 4.0min\n",
      "[CV] learning_rate=0.2, epochs=20, dropout_rate=0.6, batch_size=500 ..\n",
      "[CV]  learning_rate=0.05, epochs=20, dropout_rate=0.6, batch_size=100, total= 3.8min\n",
      "[CV] learning_rate=0.2, epochs=20, dropout_rate=0.6, batch_size=500 ..\n",
      "[CV]  learning_rate=0.05, epochs=20, dropout_rate=0.6, batch_size=100, total= 3.9min\n",
      "[CV] learning_rate=0.2, epochs=20, dropout_rate=0.6, batch_size=500 ..\n",
      "[CV]  learning_rate=0.05, epochs=20, dropout_rate=0.6, batch_size=100, total= 3.9min\n",
      "[CV] learning_rate=0.2, epochs=20, dropout_rate=0.6, batch_size=500 ..\n",
      "[CV]  learning_rate=0.2, epochs=20, dropout_rate=0.6, batch_size=500, total= 1.7min\n",
      "[CV] learning_rate=0.03, epochs=50, dropout_rate=0.8, batch_size=200 .\n",
      "[CV]  learning_rate=0.2, epochs=20, dropout_rate=0.6, batch_size=500, total= 1.7min\n",
      "[CV]  learning_rate=0.2, epochs=20, dropout_rate=0.6, batch_size=500, total= 1.7min\n",
      "[CV] learning_rate=0.03, epochs=50, dropout_rate=0.8, batch_size=200 .\n",
      "[CV] learning_rate=0.03, epochs=50, dropout_rate=0.8, batch_size=200 .\n",
      "[CV]  learning_rate=0.2, epochs=20, dropout_rate=0.6, batch_size=500, total= 2.0min\n",
      "[CV] learning_rate=0.03, epochs=50, dropout_rate=0.8, batch_size=200 .\n",
      "[CV]  learning_rate=0.03, epochs=50, dropout_rate=0.8, batch_size=200, total= 5.7min\n",
      "[CV] learning_rate=0.03, epochs=50, dropout_rate=0.0, batch_size=500 .\n",
      "[CV]  learning_rate=0.03, epochs=50, dropout_rate=0.8, batch_size=200, total= 5.8min\n",
      "[CV] learning_rate=0.03, epochs=50, dropout_rate=0.0, batch_size=500 .\n",
      "[CV]  learning_rate=0.03, epochs=50, dropout_rate=0.8, batch_size=200, total= 5.8min\n",
      "[CV] learning_rate=0.03, epochs=50, dropout_rate=0.0, batch_size=500 .\n",
      "[CV]  learning_rate=0.03, epochs=50, dropout_rate=0.8, batch_size=200, total= 6.1min\n",
      "[CV] learning_rate=0.03, epochs=50, dropout_rate=0.0, batch_size=500 .\n",
      "[CV]  learning_rate=0.03, epochs=50, dropout_rate=0.0, batch_size=500, total= 4.1min\n",
      "[CV] learning_rate=0.05, epochs=20, dropout_rate=0.8, batch_size=200 .\n",
      "[CV]  learning_rate=0.03, epochs=50, dropout_rate=0.0, batch_size=500, total= 4.1min\n",
      "[CV]  learning_rate=0.03, epochs=50, dropout_rate=0.0, batch_size=500, total= 4.1min\n",
      "[CV] learning_rate=0.05, epochs=20, dropout_rate=0.8, batch_size=200 .\n",
      "[CV] learning_rate=0.05, epochs=20, dropout_rate=0.8, batch_size=200 .\n",
      "[CV]  learning_rate=0.03, epochs=50, dropout_rate=0.0, batch_size=500, total= 4.3min\n",
      "[CV] learning_rate=0.05, epochs=20, dropout_rate=0.8, batch_size=200 .\n",
      "[CV]  learning_rate=0.05, epochs=20, dropout_rate=0.8, batch_size=200, total= 2.6min\n",
      "[CV] learning_rate=0.01, epochs=20, dropout_rate=0.8, batch_size=100 .\n",
      "[CV]  learning_rate=0.05, epochs=20, dropout_rate=0.8, batch_size=200, total= 2.5min\n",
      "[CV]  learning_rate=0.05, epochs=20, dropout_rate=0.8, batch_size=200, total= 2.5min\n",
      "[CV] learning_rate=0.01, epochs=20, dropout_rate=0.8, batch_size=100 .\n",
      "[CV] learning_rate=0.01, epochs=20, dropout_rate=0.8, batch_size=100 .\n",
      "[CV]  learning_rate=0.05, epochs=20, dropout_rate=0.8, batch_size=200, total= 2.6min\n",
      "[CV] learning_rate=0.01, epochs=20, dropout_rate=0.8, batch_size=100 .\n",
      "[CV]  learning_rate=0.01, epochs=20, dropout_rate=0.8, batch_size=100, total= 3.9min\n",
      "[CV] learning_rate=0.01, epochs=50, dropout_rate=0.6, batch_size=100 .\n",
      "[CV]  learning_rate=0.01, epochs=20, dropout_rate=0.8, batch_size=100, total= 4.0min\n",
      "[CV]  learning_rate=0.01, epochs=20, dropout_rate=0.8, batch_size=100, total= 3.9min\n",
      "[CV] learning_rate=0.01, epochs=50, dropout_rate=0.6, batch_size=100 .\n",
      "[CV] learning_rate=0.01, epochs=50, dropout_rate=0.6, batch_size=100 .\n",
      "[CV]  learning_rate=0.01, epochs=20, dropout_rate=0.8, batch_size=100, total= 4.1min\n",
      "[CV] learning_rate=0.01, epochs=50, dropout_rate=0.6, batch_size=100 .\n",
      "[CV]  learning_rate=0.01, epochs=50, dropout_rate=0.6, batch_size=100, total= 9.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 46.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=0.1, epochs=20, dropout_rate=0.8, batch_size=100 ..\n",
      "[CV]  learning_rate=0.01, epochs=50, dropout_rate=0.6, batch_size=100, total= 9.7min\n",
      "[CV] learning_rate=0.1, epochs=20, dropout_rate=0.8, batch_size=100 ..\n",
      "[CV]  learning_rate=0.01, epochs=50, dropout_rate=0.6, batch_size=100, total= 9.8min\n",
      "[CV] learning_rate=0.1, epochs=20, dropout_rate=0.8, batch_size=100 ..\n",
      "[CV]  learning_rate=0.01, epochs=50, dropout_rate=0.6, batch_size=100, total= 9.8min\n",
      "[CV] learning_rate=0.1, epochs=20, dropout_rate=0.8, batch_size=100 ..\n",
      "[CV]  learning_rate=0.1, epochs=20, dropout_rate=0.8, batch_size=100, total= 4.2min\n",
      "[CV]  learning_rate=0.1, epochs=20, dropout_rate=0.8, batch_size=100, total= 4.0min\n",
      "[CV]  learning_rate=0.1, epochs=20, dropout_rate=0.8, batch_size=100, total= 4.1min\n",
      "[CV]  learning_rate=0.1, epochs=20, dropout_rate=0.8, batch_size=100, total= 3.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed: 51.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004859565491096622\n",
      "{'learning_rate': 0.1, 'epochs': 50, 'dropout_rate': 0.8, 'batch_size': 100}\n"
     ]
    }
   ],
   "source": [
    "# grid search for optimal parameters: batch and epochs\n",
    "def create_model(layer_width=50, dropout_rate=0.2, learning_rate=0.1):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(layer_width, input_shape=(X.shape[1],X.shape[2])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    adam = Adam(lr=learning_rate, decay=0.1, amsgrad=False)\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=adam, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "np.random.seed(seed)\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [100, 200, 500]\n",
    "epochs = [20, 50, 100]\n",
    "learning_rate= [0.01,0.03,0.05, 0.1, 0.2]\n",
    "dropout_rate = [0.0, 0.2, 0.4, 0.6, 0.8]\n",
    "layer_width = [25, 50, 75, 100]\n",
    "\n",
    "# learning_rate=learning_rate\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, learning_rate=learning_rate, dropout_rate=dropout_rate)\n",
    "grid = RandomizedSearchCV(estimator=model, verbose=2, param_distributions=param_grid, cv=split, n_iter=50, n_jobs=-1)\n",
    "grid.fit(X, y, verbose=2)\n",
    "# summarize results\n",
    "\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KerasClassifier' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-8569bcdfd70c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../models/max/simple_LSTM_MAX.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#joblib.dump(grid, '../models/max/simple_LSTM_MAX.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KerasClassifier' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "grid.estimator.save('../models/max/simple_LSTM_MAX.h5')\n",
    "#joblib.dump(grid, '../models/max/simple_LSTM_MAX.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00174481],\n",
       "       [0.00174481],\n",
       "       [0.00174481],\n",
       "       ...,\n",
       "       [0.00174481],\n",
       "       [0.00174481],\n",
       "       [0.00174481]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_PATH = '../data/2018_test_org.csv'\n",
    "def load_submission_file():\n",
    "    df_test = pd.read_csv(SUBMISSION_PATH)\n",
    "    return df_test\n",
    "\n",
    "def prepare_submission_file(df_test):\n",
    "    df_stations = get_station_df()\n",
    "    df_out = add_coordinates(df_test, df_stations, src_index='ID', foreign_index='station')\n",
    "    df_out = add_day_of_year_column(df_out, column_name='DATE')\n",
    "    return df_out\n",
    "    \n",
    "def save_submission(df_src, PATH):\n",
    "    df_submission = pd.DataFrame()\n",
    "    df_submission['SUB_ID'] = df_src['DATE'].apply(lambda d: str(d)) + df_src['ID']\n",
    "    df_submission['DATA_VALUE'] = df_src['DATA_VALUE']\n",
    "    df_submission.to_csv(PATH, index=False)\n",
    "    return df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_from_simple_random_forest_model():\n",
    "    model = grid\n",
    "    required_features = [\n",
    "        'day',\n",
    "        'year',\n",
    "        'lat',\n",
    "        'long',\n",
    "        'elev',\n",
    "        ]\n",
    "    PREDICITON_FILE_PATH = '../data/predictions/prediction_simple_LSTM_MAX.csv'\n",
    "\n",
    "    df_test = load_submission_file()\n",
    "    df_test = prepare_submission_file(df_test)\n",
    "    df_test['year'] = 2018\n",
    "    df_test.head()\n",
    "\n",
    "    # create predictions\n",
    "    df_predict = df_test\n",
    "#    df_predict = df_predict.values \n",
    "    X_predict = df_predict.drop(columns=['ID', 'DATE'])\n",
    "    X_predict = X_predict.values\n",
    "    X_predict = scaler.fit_transform(X_predict)\n",
    "    X_predict = X_predict.reshape((len(X_predict),1,5))\n",
    "    df_predict['DATA_VALUE'] = scaler.inverse_transform(model.predict(X_predict)).ravel()\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #save predictions\n",
    "    df_submission = save_submission(df_predict, PREDICITON_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (397804,1) doesn't match the broadcast shape (397804,5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-a336cff92920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_predictions_from_simple_random_forest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-104-123eef4b8c0c>\u001b[0m in \u001b[0;36mgenerate_predictions_from_simple_random_forest_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mX_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mX_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdf_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATA_VALUE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/WebDev/DataMining/.dma/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (397804,1) doesn't match the broadcast shape (397804,5)"
     ]
    }
   ],
   "source": [
    "generate_predictions_from_simple_random_forest_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>elev</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASN00015643</td>\n",
       "      <td>20180101</td>\n",
       "      <td>-22.4518</td>\n",
       "      <td>133.6377</td>\n",
       "      <td>565.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASN00085296</td>\n",
       "      <td>20180101</td>\n",
       "      <td>-37.7481</td>\n",
       "      <td>147.1428</td>\n",
       "      <td>480.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASN00085280</td>\n",
       "      <td>20180101</td>\n",
       "      <td>-38.2094</td>\n",
       "      <td>146.4747</td>\n",
       "      <td>55.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA005030984</td>\n",
       "      <td>20180101</td>\n",
       "      <td>52.8167</td>\n",
       "      <td>-97.6167</td>\n",
       "      <td>223.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA003076680</td>\n",
       "      <td>20180101</td>\n",
       "      <td>55.1000</td>\n",
       "      <td>-117.2000</td>\n",
       "      <td>698.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID      DATE      lat      long   elev  day\n",
       "0  ASN00015643  20180101 -22.4518  133.6377  565.6    1\n",
       "1  ASN00085296  20180101 -37.7481  147.1428  480.0    1\n",
       "2  ASN00085280  20180101 -38.2094  146.4747   55.7    1\n",
       "3  CA005030984  20180101  52.8167  -97.6167  223.0    1\n",
       "4  CA003076680  20180101  55.1000 -117.2000  698.0    1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_submission_file(load_submission_file()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
