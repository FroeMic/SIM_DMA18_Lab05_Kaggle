{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) #to supress import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COLUMNS = ['station','date','feature', 'value', 'measurement','quality', 'source', 'hour']\n",
    "COLUMNS_test = ['station','date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_train = pd.read_csv('../data/export_features_loc_MAX.csv', index_col=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['date'] = pd.to_datetime(df_train['date'], format='%Y%m%d', errors='ignore')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you want to use past days as predictor?\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get year and date as features\n",
    "df = df_train\n",
    "df = df_train_yd.drop(columns='date')\n",
    "df['year'] = df_train['date'].map(lambda x: x.year)\n",
    "df['day'] = df_train['date'].map(lambda x: x.timetuple().tm_yday)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to reduce preproccesing duration\n",
    "#df_train_yd.to_csv('../data/tmp/export_LSMT_MAX_yd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data ready for LMST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the preprocessed data for a quicker start\n",
    "df = pd.read_csv('../data/tmp/export_LSMT_MAX_yd.csv', index_col=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick random stations for test and training\n",
    "seed = 93598357\n",
    "np.random.seed(seed)\n",
    "stations = df.station.unique()\n",
    "np.random.shuffle(stations)\n",
    "stations_shuffled = stations\n",
    "stations_train = stations_shuffled[:int(np.round(len(stations)/4))]\n",
    "stations_holdout14 = stations_shuffled[int(np.round(len(stations)/4)):int(np.round(len(stations)/2))]\n",
    "stations_holdout15 = stations_shuffled[int(np.round(len(stations)/2)):int(np.round(len(stations)/4*3))]\n",
    "stations_holdout16 = stations_shuffled[int(np.round(len(stations)/4*3)):]\n",
    "\n",
    "df_train_test = df[df['station'].isin(stations_train)]\n",
    "df_14 = df[df['station'].isin(stations_holdout14)]\n",
    "df_15 = df[df['station'].isin(stations_holdout15)]\n",
    "df_16 = df[df['station'].isin(stations_holdout16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4639908 4680661 4705872 4657383\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train_test), len(df_14), len(df_15), len(df_16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3534546, 7) (1143123, 7)\n",
      "(3534546, 7) (1143123, 7)\n",
      "(3534546, 7) (1143123, 7)\n",
      "(3534546, 7) (1143123, 7)\n"
     ]
    }
   ],
   "source": [
    "#divide test and training to test effective of model to different timeframe (start of 2017)\n",
    "training_years = [2014,2015,2016]\n",
    "testing_days = list(range(90))\n",
    "\n",
    "df_train = df_train_test[df_train_test['year'].isin(training_years)]\n",
    "df_test = df_train_test[~df_train_test['year'].isin(testing_days)]\n",
    "df_test = df_test[df_test['day'].isin(testing_days)]\n",
    "print(df_train.shape,df_test.shape)\n",
    "\n",
    "training_years = [2017,2015,2016]\n",
    "df_train14 = df_14[df_14['year'].isin(training_years)]\n",
    "df_test14 = df_14[~df_14['year'].isin(testing_days)]\n",
    "df_test14 = df_test14[df_test14['day'].isin(testing_days)]\n",
    "print(df_train.shape,df_test.shape)\n",
    "\n",
    "training_years = [2017,2014,2016]\n",
    "df_train15 = df_15[df_15['year'].isin(training_years)]\n",
    "df_test15 = df_15[~df_15['year'].isin(testing_days)]\n",
    "df_test15 = df_test15[df_test15['day'].isin(testing_days)]\n",
    "print(df_train.shape,df_test.shape)\n",
    "\n",
    "training_years = [2017,2015,2014]\n",
    "df_train16 = df_16[df_16['year'].isin(training_years)]\n",
    "df_test16 = df_16[~df_16['year'].isin(testing_days)]\n",
    "df_test16 = df_test16[df_test16['day'].isin(testing_days)]\n",
    "print(df_train.shape,df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export model data for the cloud training\n",
    "df_train.to_csv('../data/aws_train')\n",
    "df_test.to_csv('../data/aws_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train14.to_csv('../data/aws_train14')\n",
    "df_test14.to_csv('../data/aws_test14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train15.to_csv('../data/aws_train15')\n",
    "df_test15.to_csv('../data/aws_test15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train16.to_csv('../data/aws_train16')\n",
    "df_test16.to_csv('../data/aws_test16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate target from features\n",
    "df_X_train_raw = df_train.drop(columns='TMIN')\n",
    "df_X_test_raw = df_test.drop(columns='TMIN')\n",
    "sy_train = df_train['TMIN'].values\n",
    "sy_test = df_test['TMIN'].values\n",
    "y_train_raw = sy_train.reshape(-1,1)\n",
    "y_test_raw = sy_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int encode stations\n",
    "#LB = LabelBinarizer()\n",
    "#df_X['station'] = LB.fit_transform(df_X[['station']])\n",
    "df_X_train_red = df_X_train_raw.drop(columns='station')\n",
    "df_X_test_red = df_X_test_raw.drop(columns='station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_dict = df_X.to_dict('records')\n",
    "#vec = DictVectorizer()\n",
    "#X = vec.fit_transform(X_dict).toarray()\n",
    "#X_dummies = pd.get_dummies(df_X)\n",
    "#X = X_dummies.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features\n",
    "X_train_raw = df_X_train_red.values\n",
    "X_test_raw = df_X_test_red.values\n",
    "X_train_raw = X_train_raw.astype('float32')\n",
    "X_test_raw = X_test_raw.astype('float32')\n",
    "y_train_raw = y_train_raw.astype('float32')\n",
    "y_test_raw = y_test_raw.astype('float32')\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))                             \n",
    "train_X = scaler.fit_transform(X_train_raw)\n",
    "test_X = scaler.fit_transform(X_test_raw)\n",
    "train_y = scaler.fit_transform(y_train_raw)\n",
    "test_y = scaler.fit_transform(y_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.shape,train_y.shape,test_X.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the number of lag days\n",
    "n_days = 0\n",
    "n_features = 1\n",
    "# frame as supervised learning\n",
    "#reframed = scaled #series_to_supervised(scaled, n_days, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((1792641,1,5))\n",
    "test_X = test_X.reshape((514252,1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design network\n",
    "learning_rate = 0.2\n",
    "epochs = 50\n",
    "decay_rate = learning_rate / epochs\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1],train_X.shape[2])))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(1))\n",
    "adam = Adam(lr = learning_rate, decay=decay_rate)\n",
    "model.compile(loss='mae', optimizer=adam)\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=epochs, batch_size=200, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = np.concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = np.concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for optimal parameters: batch and epochs\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(train_X.shape[1],train_X.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "np.random.seed(seed)\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 20, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(train_X, train_y).score(X_test, y_test)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for optimal parameters: dropout rate\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(train_X.shape[1],train_X.shape[2])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "np.random.seed(seed)\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = \n",
    "epochs = \n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(dropout_rate=dropout_rate)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(train_X, train_y).score(X_test, y_test)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = np.concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = np.concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kaggle scoring data\n",
    "df_score = pd.read_csv('../data/2018_test.csv', header=None, names=COLUMNS_test, low_memory=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
