{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer, normalize\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) #to supress import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data ready for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the preprocessed data for a quicker start\n",
    "\n",
    "TRAIN_FILE = '../data/tmp/export_LSMT_MAX_yd.csv'\n",
    "STATIONS_FILE = '../data/ghcnd-stations.csv'\n",
    "\n",
    "def get_original_df():\n",
    "    df_original = pd.read_csv(TRAIN_FILE)\n",
    "    df_original = df_original.drop(['Unnamed: 0'], axis=1)\n",
    "    return df_original\n",
    "\n",
    "def get_station_df():\n",
    "     return pd.read_csv(STATIONS_FILE, header=None, names=['station','lat', 'long', 'elev'], sep=';')\n",
    "\n",
    "def add_coordinates(df_src, df_stations, src_index='station', foreign_index='station'):\n",
    "    df_out = df_src.copy()\n",
    "    return df_out.join(df_stations.set_index(foreign_index), on=src_index)\n",
    "\n",
    "\n",
    "def add_day_of_year_column(df_src, column_name='date'):\n",
    "    df_out = df_src.copy()\n",
    "    df_out['day'] = df_out[column_name].apply(lambda d: date_to_nth_day(str(d)))\n",
    "    return df_out\n",
    "\n",
    "def date_to_nth_day(date, format='%Y%m%d'):\n",
    "    date = datetime.strptime(date, format)\n",
    "    new_year_day = datetime(year=date.year, month=1, day=1)\n",
    "    return (date - new_year_day).days + 1\n",
    "\n",
    "def select_max(results):\n",
    "    (best_k, best_score) = 0,0\n",
    "    for (k, s) in results:\n",
    "        if s > best_score:\n",
    "            best_k = k\n",
    "            best_score = s\n",
    "    return (best_k, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/tmp/export_LSMT_MAX_yd.csv', index_col=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>elev</th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>128</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>145</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>140</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>162</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>115</td>\n",
       "      <td>25.333</td>\n",
       "      <td>55.517</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       station  TMIN     lat    long  elev  year  day\n",
       "0  AE000041196   128  25.333  55.517  34.0  2014    1\n",
       "1  AE000041196   145  25.333  55.517  34.0  2014    2\n",
       "2  AE000041196   140  25.333  55.517  34.0  2014    3\n",
       "3  AE000041196   162  25.333  55.517  34.0  2014    6\n",
       "4  AE000041196   115  25.333  55.517  34.0  2014    9"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick random stations for test and training\n",
    "seed = 93598357\n",
    "np.random.seed(seed)\n",
    "stations = df.station.unique()\n",
    "np.random.shuffle(stations)\n",
    "stations_shuffled = stations\n",
    "fraction = 4\n",
    "stations_train = stations_shuffled[:int(np.round(len(stations)/fraction))]\n",
    "stations_holdout14 = stations_shuffled[int(np.round(len(stations)/fraction)):int(np.round(len(stations)/fraction*2))]\n",
    "stations_holdout15 = stations_shuffled[int(np.round(len(stations)/fraction*2)):int(np.round(len(stations)/fraction*3))]\n",
    "stations_holdout16 = stations_shuffled[int(np.round(len(stations)/fraction*3)):int(np.round(len(stations)/fraction*4))]\n",
    "\n",
    "df_17 = df#[df['station'].isin(stations_train)]\n",
    "df_14 = df#[df['station'].isin(stations_holdout14)]\n",
    "df_15 = df#[df['station'].isin(stations_holdout15)]\n",
    "df_16 = df#[df['station'].isin(stations_holdout16)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18683824 18683824 18683824 18683824\n"
     ]
    }
   ],
   "source": [
    "print(len(df_17), len(df_14), len(df_15), len(df_16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4591256, 7)\n"
     ]
    }
   ],
   "source": [
    "#divide test and training to test effective of model to different timeframe (start of 2017)\n",
    "testing_days = list(range(90))\n",
    "\n",
    "df_train = df[df['day'].isin(testing_days)]\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3481227, 7) (1110029, 7)\n",
      "(3412382, 7) (1178874, 7)\n",
      "(3428517, 7) (1162739, 7)\n",
      "(3451642, 7) (1139614, 7)\n"
     ]
    }
   ],
   "source": [
    "#divide test and training to test effective of model to different timeframe (start of 2017)\n",
    "training_years = [2014,2015,2016]\n",
    "testing_days = list(range(90))\n",
    "\n",
    "df_train17 = df_17[df_17['year'].isin(training_years)]\n",
    "df_train17 = df_train17[df_train17['day'].isin(testing_days)]\n",
    "df_test17 = df_17[~df_17['year'].isin(training_years)]\n",
    "df_test17 = df_test17[df_test17['day'].isin(testing_days)]\n",
    "print(df_train17.shape,df_test17.shape)\n",
    "\n",
    "training_years = [2017,2015,2016]\n",
    "df_train14 = df_14[df_14['year'].isin(training_years)]\n",
    "df_train14 = df_train14[df_train14['day'].isin(testing_days)]\n",
    "df_test14 = df_14[~df_14['year'].isin(training_years)]\n",
    "df_test14 = df_test14[df_test14['day'].isin(testing_days)]\n",
    "print(df_train14.shape,df_test14.shape)\n",
    "\n",
    "training_years = [2017,2014,2016]\n",
    "df_train15 = df_15[df_15['year'].isin(training_years)]\n",
    "df_train15 = df_train15[df_train15['day'].isin(testing_days)]\n",
    "df_test15 = df_15[~df_15['year'].isin(training_years)]\n",
    "df_test15 = df_test15[df_test15['day'].isin(testing_days)]\n",
    "print(df_train15.shape,df_test15.shape)\n",
    "\n",
    "training_years = [2017,2015,2014]\n",
    "df_train16 = df_16[df_16['year'].isin(training_years)]\n",
    "df_train16 = df_train16[df_train16['day'].isin(testing_days)]\n",
    "df_test16 = df_16[~df_16['year'].isin(training_years)]\n",
    "df_test16 = df_test16[df_test16['day'].isin(testing_days)]\n",
    "print(df_train16.shape,df_test16.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define split for CV later on\n",
    "split = [[df_train17.index.values, df_test17.index.values], [df_train16.index.values, df_test16.index.values],\n",
    "         [df_train15.index.values, df_test15.index.values],[df_train14.index.values, df_test14.index.values]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate target from features\n",
    "df_X_raw = df_train.drop(columns='TMIN')\n",
    "df_X_raw_cv = df.drop(columns='TMIN')\n",
    "sy = df_train['TMIN']\n",
    "sy_cv = df['TMIN']\n",
    "y_raw = sy.reshape(-1,1)\n",
    "y_raw_cv = sy_cv.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4591256, 6)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int encode stations\n",
    "#LB = LabelBinarizer()\n",
    "#df_X['station'] = LB.fit_transform(df_X[['station']])\n",
    "df_X_red = df_X_raw.drop(columns='station')\n",
    "df_X_red_cv = df_X_raw_cv.drop(columns='station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_dict = df_X.to_dict('records')\n",
    "#vec = DictVectorizer()\n",
    "#X = vec.fit_transform(X_dict).toarray()\n",
    "#X_dummies = pd.get_dummies(df_X)\n",
    "#X = X_dummies.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features\n",
    "X_raw = df_X_red.values\n",
    "X_raw_cv = df_X_red_cv.values\n",
    "y_raw = y_raw.astype('float32')\n",
    "y_raw_cv = y_raw_cv.astype('float32')\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))                             \n",
    "X = scaler.fit_transform(X_raw)\n",
    "X_cv = scaler.fit_transform(X_raw_cv)\n",
    "y = scaler.fit_transform(y_raw).ravel()\n",
    "y_cv = scaler.fit_transform(y_raw_cv).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 10,\n",
       " 'max_features': 3,\n",
       " 'min_samples_leaf': 10,\n",
       " 'min_samples_split': 10,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 3, 'max_depth': 10, 'bootstrap': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0023            5.46m\n",
      "         2           0.0019            4.14m\n",
      "         3           0.0017            3.60m\n",
      "         4           0.0016            3.54m\n",
      "         5           0.0015            3.46m\n",
      "         6           0.0014            3.99m\n",
      "         7           0.0013            3.74m\n",
      "         8           0.0012            3.58m\n",
      "         9           0.0012            3.45m\n",
      "        10           0.0011            3.32m\n",
      "        11           0.0011            3.42m\n",
      "        12           0.0010            3.29m\n",
      "        13           0.0010            3.50m\n",
      "        14           0.0010            3.45m\n",
      "        15           0.0009            3.38m\n",
      "        16           0.0009            3.61m\n",
      "        17           0.0009            3.84m\n",
      "        18           0.0009            3.72m\n",
      "        19           0.0008            3.66m\n",
      "        20           0.0008            3.74m\n",
      "        21           0.0008            3.66m\n",
      "        22           0.0008            3.60m\n",
      "        23           0.0008            3.55m\n",
      "        24           0.0008            3.43m\n",
      "        25           0.0008            3.39m\n",
      "        26           0.0008            3.40m\n",
      "        27           0.0007            3.41m\n",
      "        28           0.0007            3.47m\n",
      "        29           0.0007            3.44m\n",
      "        30           0.0007            3.40m\n",
      "        31           0.0007            3.44m\n",
      "        32           0.0007            3.44m\n",
      "        33           0.0007            3.43m\n",
      "        34           0.0007            3.36m\n",
      "        35           0.0007            3.30m\n",
      "        36           0.0007            3.24m\n",
      "        37           0.0007            3.17m\n",
      "        38           0.0007            3.11m\n",
      "        39           0.0007            3.06m\n",
      "        40           0.0007            3.07m\n",
      "        41           0.0007            3.04m\n",
      "        42           0.0007            3.03m\n",
      "        43           0.0007            3.02m\n",
      "        44           0.0007            2.97m\n",
      "        45           0.0006            2.92m\n",
      "        46           0.0006            2.89m\n",
      "        47           0.0006            2.84m\n",
      "        48           0.0006            2.81m\n",
      "        49           0.0006            2.77m\n",
      "        50           0.0006            2.73m\n",
      "        51           0.0006            2.67m\n",
      "        52           0.0006            2.60m\n",
      "        53           0.0006            2.56m\n",
      "        54           0.0006            2.52m\n",
      "        55           0.0006            2.46m\n",
      "        56           0.0006            2.42m\n",
      "        57           0.0006            2.38m\n",
      "        58           0.0006            2.33m\n",
      "        59           0.0006            2.28m\n",
      "        60           0.0006            2.23m\n",
      "        61           0.0006            2.20m\n",
      "        62           0.0006            2.16m\n",
      "        63           0.0006            2.12m\n",
      "        64           0.0006            2.09m\n",
      "        65           0.0006            2.10m\n",
      "        66           0.0006            2.05m\n",
      "        67           0.0006            2.01m\n",
      "        68           0.0006            1.97m\n",
      "        69           0.0006            1.93m\n",
      "        70           0.0006            1.91m\n",
      "        71           0.0006            1.88m\n",
      "        72           0.0006            1.85m\n",
      "        73           0.0006            1.81m\n",
      "        74           0.0006            1.77m\n",
      "        75           0.0006            1.73m\n",
      "        76           0.0006            1.71m\n",
      "        77           0.0006            1.67m\n",
      "        78           0.0006            1.64m\n",
      "        79           0.0006            1.61m\n",
      "        80           0.0006            1.57m\n",
      "        81           0.0006            1.53m\n",
      "        82           0.0006            1.50m\n",
      "        83           0.0006            1.46m\n",
      "        84           0.0006            1.42m\n",
      "        85           0.0006            1.38m\n",
      "        86           0.0006            1.34m\n",
      "        87           0.0006            1.31m\n",
      "        88           0.0006            1.27m\n",
      "        89           0.0006            1.24m\n",
      "        90           0.0006            1.21m\n",
      "        91           0.0006            1.17m\n",
      "        92           0.0006            1.15m\n",
      "        93           0.0006            1.11m\n",
      "        94           0.0006            1.08m\n",
      "        95           0.0006            1.05m\n",
      "        96           0.0006            1.01m\n",
      "        97           0.0006           58.72s\n",
      "        98           0.0006           56.76s\n",
      "        99           0.0006           54.67s\n",
      "       100           0.0006           52.34s\n",
      "       101           0.0006           50.10s\n",
      "       102           0.0006           47.96s\n",
      "       103           0.0006           45.94s\n",
      "       104           0.0006           43.78s\n",
      "       105           0.0006           41.64s\n",
      "       106           0.0006           39.61s\n",
      "       107           0.0006           37.43s\n",
      "       108           0.0006           35.42s\n",
      "       109           0.0006           33.35s\n",
      "       110           0.0006           32.14s\n",
      "       111           0.0006           29.95s\n",
      "       112           0.0006           27.76s\n",
      "       113           0.0006           25.63s\n",
      "       114           0.0006           23.38s\n",
      "       115           0.0006           21.20s\n",
      "       116           0.0006           19.08s\n",
      "       117           0.0006           16.87s\n",
      "       118           0.0006           14.72s\n",
      "       119           0.0006           12.57s\n",
      "       120           0.0006           10.43s\n",
      "       121           0.0006            8.32s\n",
      "       122           0.0006            6.21s\n",
      "       123           0.0006            4.14s\n",
      "       124           0.0006            2.06s\n",
      "       125           0.0006            0.00s\n",
      "0.7776073436330675\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingRegressor(n_estimators= 125, learning_rate=0.2, max_depth=5, max_features=3, min_samples_split=10, \n",
    "                                  min_samples_leaf=10, random_state=0, verbose=2)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/max/boosting_.pkl']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, '../models/max/boosting_.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_PATH = '../data/2018_test_org.csv'\n",
    "def load_submission_file():\n",
    "    df_test = pd.read_csv(SUBMISSION_PATH)\n",
    "    return df_test\n",
    "\n",
    "def prepare_submission_file(df_test):\n",
    "    df_stations = get_station_df()\n",
    "    df_out = add_coordinates(df_test, df_stations, src_index='ID', foreign_index='station')\n",
    "    df_out = add_day_of_year_column(df_out, column_name='DATE')\n",
    "    return df_out\n",
    "    \n",
    "def save_submission(df_src, PATH):\n",
    "    df_submission = pd.DataFrame()\n",
    "    df_submission['SUB_ID'] = df_src['DATE'].apply(lambda d: str(d)) + df_src['ID']\n",
    "    df_submission['DATA_VALUE'] = df_src['DATA_VALUE']\n",
    "    df_submission.to_csv(PATH, index=False)\n",
    "    return df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_from_simple_random_forest_model():\n",
    "    model = joblib.load('../models/max/boosting_.pkl')\n",
    "    required_features = [\n",
    "        'day',\n",
    "        'year',\n",
    "        'lat',\n",
    "        'long',\n",
    "        'elev',\n",
    "        ]\n",
    "    PREDICITON_FILE_PATH = '../data/predictions/prediction_boosting__MAX.csv'\n",
    "\n",
    "    df_test = load_submission_file()\n",
    "    df_test = prepare_submission_file(df_test)\n",
    "    df_test['year'] = 2018\n",
    "    df_test.head()\n",
    "\n",
    "    # create predictions\n",
    "    df_predict = df_test\n",
    "    df_predict['DATA_VALUE'] = model.predict(df_test[required_features])\n",
    "\n",
    "    #save predictions\n",
    "    df_submission = save_submission(df_predict, PREDICITON_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_predictions_from_simple_random_forest_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
