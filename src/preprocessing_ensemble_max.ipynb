{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "\n",
    "You don't have to run **Step 1** if there is the *../data/tmp/df_time.csv* file exists.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = ['station','date','feature', 'value', 'measurement','quality', 'source', 'hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/2014.csv', header=None, names=COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(pd.read_csv('../data/2015.csv', header=None, names=COLUMNS))\n",
    "df = df.append(pd.read_csv('../data/2016.csv', header=None, names=COLUMNS))\n",
    "df = df.append(pd.read_csv('../data/2017.csv', header=None, names=COLUMNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Only selecting features who are available in Test Set + Feature to be predicted\n",
    "selected_features = ['TMIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['feature'].isin(selected_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df.pivot_table(index=['station','date'], columns='feature', values='value', aggfunc=np.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df_pivot['TMIN']\n",
    "df_time = df_time.reset_index()\n",
    "len(df_time)\n",
    "\n",
    "# Export df_time, so we don't have to load all of that data all the time.\n",
    "#df_time.to_csv('../data/tmp/df_time.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Make sure you have loaded `df_time` either in **Step 1** or you subsequently load it from a saved file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_path = '../data/tmp/df_time.csv'\n",
    "df_time = pd.read_csv(df_time_path)\n",
    "df_time = df_time.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include Location Information?\n",
    "#df_stations = pd.read_csv('../data/ghcnd-stations.csv', header=None, names=['station','lat', 'long', 'elev'], sep=';')\n",
    "#df_stations = df_stations.set_index('station')\n",
    "stations = df_time.station.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_all_nth_day_features(input_data, feature, N, export_file):\n",
    "    '''\n",
    "        Generates a new dataframe with <feature>_1 .. <feature>_N columns\n",
    "        which represent the feature values of the previous N days.\n",
    "    \n",
    "        @param input_data:  Pandas dataframe with columns \n",
    "                            'station', 'date' and <feature>.\n",
    "                            Needs to be ordered by station and date.\n",
    "        @param feature:     Name of the feature column that should be used.\n",
    "        @param N:           How many feature columns should be generated.\n",
    "        @param_export_file  The open csv. file to which should be exported.\n",
    "    '''\n",
    "    \n",
    "    # avoid side effects on input dataframe\n",
    "    data = input_data.copy()\n",
    "    \n",
    "    # add empty columns\n",
    "    col_name_for = lambda n: \"{}_{}\".format(feature, n)\n",
    "    for i in range(1, N+1):\n",
    "        data[col_name_for(i)] = [None]*len(data)\n",
    "        \n",
    "    # create empty dataframe for export\n",
    "    SHOULD_WRITE_HEADERS = True\n",
    "    COLUMN_NAMES = list(df_time.columns.values)\n",
    "    export_data_frame = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "\n",
    "    # iterate over all stations and write compute the new columns\n",
    "    for i, station in enumerate(stations):\n",
    "        rows_for_station = data[data['station']==station]\n",
    "        first_index = rows_for_station.index[0]\n",
    "        \n",
    "        # add verbose output to see whether stuff is still running\n",
    "        print (''*20, '\\r', end='')\n",
    "        print (i+1, '/', len(stations), '::', station, '\\r', end='')\n",
    "        \n",
    "        for row_index in range(first_index, len(rows_for_station)):\n",
    "            for i in range(1, N+1):\n",
    "                if (row_index - i >= first_index):\n",
    "                    data.at[row_index, col_name_for(i)] = data.loc[row_index-i,feature]\n",
    "        \n",
    "        # append to export frame\n",
    "        export_data_frame = export_data_frame.append( data.iloc[first_index:first_index+len(rows_for_station)])\n",
    "        \n",
    "        # Note: writing data takes very long, so only write every 2000 stations\n",
    "        SAVE_EVERY_N_STATIONS = 250\n",
    "        if (i%SAVE_EVERY_N_STATIONS == 0):\n",
    "            print(i, 'w', end='')\n",
    "            \n",
    "            export_data_frame.to_csv(f, header=SHOULD_WRITE_HEADERS)\n",
    "            \n",
    "            # only write headers in the first run\n",
    "            SHOULD_WRITE_HEADERS = False\n",
    "\n",
    "            # empty the export_data_frame again\n",
    "            export_data_frame = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "        \n",
    "    print('')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15693 / 15693 :: ZI000067983                                                             \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>date</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>TMIN_1</th>\n",
       "      <th>TMIN_2</th>\n",
       "      <th>TMIN_3</th>\n",
       "      <th>TMIN_4</th>\n",
       "      <th>TMIN_5</th>\n",
       "      <th>TMIN_6</th>\n",
       "      <th>TMIN_7</th>\n",
       "      <th>...</th>\n",
       "      <th>TMIN_11</th>\n",
       "      <th>TMIN_12</th>\n",
       "      <th>TMIN_13</th>\n",
       "      <th>TMIN_14</th>\n",
       "      <th>TMIN_15</th>\n",
       "      <th>TMIN_16</th>\n",
       "      <th>TMIN_17</th>\n",
       "      <th>TMIN_18</th>\n",
       "      <th>TMIN_19</th>\n",
       "      <th>TMIN_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>20140101</td>\n",
       "      <td>128</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>20140102</td>\n",
       "      <td>145</td>\n",
       "      <td>128</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>20140103</td>\n",
       "      <td>140</td>\n",
       "      <td>145</td>\n",
       "      <td>128</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>20140106</td>\n",
       "      <td>162</td>\n",
       "      <td>140</td>\n",
       "      <td>145</td>\n",
       "      <td>128</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AE000041196</td>\n",
       "      <td>20140109</td>\n",
       "      <td>115</td>\n",
       "      <td>162</td>\n",
       "      <td>140</td>\n",
       "      <td>145</td>\n",
       "      <td>128</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       station      date  TMIN TMIN_1 TMIN_2 TMIN_3 TMIN_4 TMIN_5 TMIN_6  \\\n",
       "0  AE000041196  20140101   128   None   None   None   None   None   None   \n",
       "1  AE000041196  20140102   145    128   None   None   None   None   None   \n",
       "2  AE000041196  20140103   140    145    128   None   None   None   None   \n",
       "3  AE000041196  20140106   162    140    145    128   None   None   None   \n",
       "4  AE000041196  20140109   115    162    140    145    128   None   None   \n",
       "\n",
       "  TMIN_7   ...   TMIN_11 TMIN_12 TMIN_13 TMIN_14 TMIN_15 TMIN_16 TMIN_17  \\\n",
       "0   None   ...      None    None    None    None    None    None    None   \n",
       "1   None   ...      None    None    None    None    None    None    None   \n",
       "2   None   ...      None    None    None    None    None    None    None   \n",
       "3   None   ...      None    None    None    None    None    None    None   \n",
       "4   None   ...      None    None    None    None    None    None    None   \n",
       "\n",
       "  TMIN_18 TMIN_19 TMIN_20  \n",
       "0    None    None    None  \n",
       "1    None    None    None  \n",
       "2    None    None    None  \n",
       "3    None    None    None  \n",
       "4    None    None    None  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 20\n",
    "tmp_file_path = '../data/tmp/export_features_2014_to_2017_{}.csv'.format(N)\n",
    "file_path = '../data/export_features_2014_to_2017_{}.csv'.format(N)\n",
    "with open(tmp_file_path, 'a') as f:\n",
    "\n",
    "    df_train = derive_all_nth_day_features(df_time, 'TMIN', N, f)\n",
    "    f.close()\n",
    "    \n",
    "    # write entire set\n",
    "    df_train.to_csv(file_path)\n",
    "    \n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simplify everything\n",
    "col_name_for = lambda n: \"{}_{}\".format('TMIN', n)\n",
    "\n",
    "df_train = df_time.copy()\n",
    "\n",
    "def derive_all_nth_day_features(input_data, feature, N, export_file):\n",
    "    \n",
    "    # avoid side effects on input dataframe\n",
    "    data = input_data.copy()\n",
    "    \n",
    "    # add empty columns\n",
    "    col_name_for = lambda n: \"{}_{}\".format(feature, n)\n",
    "    for i in range(1, N+1):\n",
    "        data[col_name_for(i)] = [None]*len(data)\n",
    "        \n",
    "    # create empty dataframe for export\n",
    "    SHOULD_WRITE_HEADERS = True\n",
    "    COLUMN_NAMES = list(df_time.columns.values)\n",
    "    export_data_frame = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "    \n",
    "    for i, station in enumerate(stations):\n",
    "        rows_for_station = data[data['station']==station]\n",
    "        first_index = rows_for_station.index[0]\n",
    "        \n",
    "        # add verbose output to see whether stuff is still running\n",
    "        print (''*20, '\\r', end='')\n",
    "        print (i+1, '/', len(stations), '::', station, '\\r', end='')\n",
    "        for n in range(1, N+1):\n",
    "            df_train[df_time['station']==station][col_name_for(n)] = df_time[df_time['station']==station]['TMIN'].shift(n)\n",
    "        \n",
    "        # append to export frame\n",
    "        export_data_frame = export_data_frame.append( data.iloc[first_index:first_index+len(rows_for_station)])\n",
    "        \n",
    "        # Note: writing data takes very long, so only write every 2000 stations\n",
    "        SAVE_EVERY_N_STATIONS = 250\n",
    "        if (i%SAVE_EVERY_N_STATIONS == 0):\n",
    "            print(i, 'w', end='')\n",
    "            \n",
    "            export_data_frame.to_csv(f, header=SHOULD_WRITE_HEADERS)\n",
    "            \n",
    "            # only write headers in the first run\n",
    "            SHOULD_WRITE_HEADERS = False\n",
    "\n",
    "            # empty the export_data_frame again\n",
    "            export_data_frame = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "        \n",
    "    print('')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "tmp_file_path = '../data/tmp/export_features_2014_to_2017_{}.csv'.format(N)\n",
    "file_path = '../data/export_features_2014_to_2017_{}.csv'.format(N)\n",
    "with open(tmp_file_path, 'a') as f:\n",
    "\n",
    "    df_train = derive_all_nth_day_features(df_time, 'TMIN', N, f)\n",
    "    f.close()\n",
    "    \n",
    "    # write entire set\n",
    "    df_train.to_csv(file_path)\n",
    "    \n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/export_features_2014_to_2017_{}.csv'.format(N)\n",
    "df_train.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
