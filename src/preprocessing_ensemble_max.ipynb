{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "\n",
    "You don't have to run **Step 1** if there is the *../data/tmp/df_time.csv* file exists.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = ['station','date','feature', 'value', 'measurement','quality', 'source', 'hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/2014.csv', header=None, names=COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(pd.read_csv('../data/2015.csv', header=None, names=COLUMNS))\n",
    "df = df.append(pd.read_csv('../data/2016.csv', header=None, names=COLUMNS))\n",
    "df = df.append(pd.read_csv('../data/2017.csv', header=None, names=COLUMNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>date</th>\n",
       "      <th>feature</th>\n",
       "      <th>value</th>\n",
       "      <th>measurement</th>\n",
       "      <th>quality</th>\n",
       "      <th>source</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASN00015643</td>\n",
       "      <td>20140101</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASN00015643</td>\n",
       "      <td>20140101</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASN00015643</td>\n",
       "      <td>20140101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US1MNCV0008</td>\n",
       "      <td>20140101</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US1MNCV0008</td>\n",
       "      <td>20140101</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       station      date feature  value measurement quality source  hour\n",
       "0  ASN00015643  20140101    TMAX    424         NaN     NaN      a   NaN\n",
       "1  ASN00015643  20140101    TMIN    227         NaN     NaN      a   NaN\n",
       "2  ASN00015643  20140101    PRCP      0         NaN     NaN      a   NaN\n",
       "3  US1MNCV0008  20140101    PRCP      0         NaN     NaN      N   NaN\n",
       "4  US1MNCV0008  20140101    SNOW      0         NaN     NaN      N   NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Only selecting features who are available in Test Set + Feature to be predicted\n",
    "selected_features = ['TMIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['feature'].isin(selected_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = df.pivot_table(index=['station','date'], columns='feature', values='value', aggfunc=np.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df_pivot['TMIN']\n",
    "df_time = df_time.reset_index()\n",
    "len(df_time)\n",
    "\n",
    "# Export df_time, so we don't have to load all of that data all the time.\n",
    "# df_time.to_csv('../data/tmp/df_time.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Make sure you have loaded `df_time` either in **Step 1** or you subsequently load it from a saved file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_path = '../data/tmp/df_time.csv'\n",
    "df_time = pd.read_csv(df_time_path)\n",
    "df_time = df_time.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include Location Information?\n",
    "df_stations = pd.read_csv('../data/ghcnd-stations.csv', header=None, names=['station','lat', 'long', 'elev'], sep=';')\n",
    "df_stations = df_stations.set_index('station')\n",
    "stations = df_time.station.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_all_nth_day_features(input_data, feature, N, export_file):\n",
    "    '''\n",
    "        Generates a new dataframe with <feature>_1 .. <feature>_N columns\n",
    "        which represent the feature values of the previous N days.\n",
    "    \n",
    "        @param input_data:  Pandas dataframe with columns \n",
    "                            'station', 'date' and <feature>.\n",
    "                            Needs to be ordered by station and date.\n",
    "        @param feature:     Name of the feature column that should be used.\n",
    "        @param N:           How many feature columns should be generated.\n",
    "        @param_export_file  The open csv. file to which should be exported.\n",
    "    '''\n",
    "    \n",
    "    # avoid side effects on input dataframe\n",
    "    data = input_data.copy()\n",
    "    \n",
    "    # add empty columns\n",
    "    col_name_for = lambda n: \"{}_{}\".format(feature, n)\n",
    "    for i in range(1, N+1):\n",
    "        data[col_name_for(i)] = [None]*len(data)\n",
    "        \n",
    "    # create empty dataframe for export\n",
    "    SHOULD_WRITE_HEADERS = True\n",
    "    COLUMN_NAMES = list(df_time.columns.values)\n",
    "    export_data_frame = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "\n",
    "    # iterate over all stations and write compute the new columns\n",
    "    for i, station in enumerate(stations):\n",
    "        rows_for_station = data[data['station']==station]\n",
    "        first_index = rows_for_station.index[0]\n",
    "        \n",
    "        # add verbose output to see whether stuff is still running\n",
    "        print (''*20, '\\r', end='')\n",
    "        print (i+1, '/', len(stations), '::', station, '\\r', end='')\n",
    "        \n",
    "        for row_index in range(first_index, len(rows_for_station)):\n",
    "            for i in range(1, N+1):\n",
    "                if (row_index - i >= first_index):\n",
    "                    data.at[row_index, col_name_for(i)] = data.loc[row_index-i,feature]\n",
    "        \n",
    "        # append to export frame\n",
    "        export_data_frame = export_data_frame.append( data.iloc[first_index:first_index+len(rows_for_station)])\n",
    "        \n",
    "        # Note: writing data takes very long, so only write every 2000 stations\n",
    "        SAVE_EVERY_N_STATIONS = 250\n",
    "        if (i%SAVE_EVERY_N_STATIONS == 0):\n",
    "            print(i, 'w', end='')\n",
    "            \n",
    "            export_data_frame.to_csv(f, header=SHOULD_WRITE_HEADERS)\n",
    "            \n",
    "            # only write headers in the first run\n",
    "            SHOULD_WRITE_HEADERS = False\n",
    "\n",
    "            # empty the export_data_frame again\n",
    "            export_data_frame = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "        \n",
    "    print('')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 / 15693 :: AGM00060369     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-c21ad558b999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mderive_all_nth_day_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TMIN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-945b8fc5e789>\u001b[0m in \u001b[0;36mderive_all_nth_day_features\u001b[0;34m(input_data, feature, N, export_file)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# iterate over all stations and write compute the new columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mrows_for_station\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'station'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mstation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mfirst_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows_for_station\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Repositories/SIM/DataMining/Assignments/05_Lab05/.lab05/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 raise TypeError('Could not compare {typ} type with Series'\n",
      "\u001b[0;32m~/Desktop/Repositories/SIM/DataMining/Assignments/05_Lab05/.lab05/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_comp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Repositories/SIM/DataMining/Assignments/05_Lab05/.lab05/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m_comp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "tmp_file_path = '../data/tmp/export_features_2014_to_2017_{}.csv'.format(N)\n",
    "file_path = '../data/export_features_2014_to_2017_{}.csv'.format(N)\n",
    "with open(tmp_file_path, 'a') as f:\n",
    "\n",
    "    df_train = derive_all_nth_day_features(df_time, 'TMIN', N, f)\n",
    "    f.close()\n",
    "    \n",
    "    # write entire set\n",
    "    df_train.to_csv(file_path)\n",
    "    \n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
